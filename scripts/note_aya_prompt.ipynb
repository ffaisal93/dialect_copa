{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d005aca2-387c-42eb-a08f-fa31a80e36b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import logging\n",
    "from generate_utility import *\n",
    "from utility import *\n",
    "from transformers.generation import GenerationConfig\n",
    "from peft import PeftModel\n",
    "# import bitsandbytes as bnb\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374feedc-541d-403f-a4f3-d579b2d03580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [01:42<00:00,  9.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"../models/aya-101\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1cd83f0-ff64-43b2-a179-ef7818bcf690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sent_model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6812c35c-922d-46b6-ae94-f5a475e2af4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n",
      "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6788\n",
      "I love pasta \t\t Do you like pizza? \t\t Score: 0.5096\n",
      "I love pasta \t\t The new movie is so great \t\t Score: 0.2560\n",
      "I love pasta \t\t The new movie is awesome \t\t Score: 0.2440\n",
      "A man is playing guitar \t\t The cat plays in the garden \t\t Score: 0.2105\n",
      "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1969\n",
      "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1692\n",
      "The cat sits outside \t\t A woman watches TV \t\t Score: 0.1310\n",
      "The cat plays in the garden \t\t Do you like pizza? \t\t Score: 0.0900\n",
      "The cat plays in the garden \t\t A woman watches TV \t\t Score: 0.0629\n",
      "A woman watches TV \t\t Do you like pizza? \t\t Score: 0.0417\n",
      "The cat sits outside \t\t A man is playing guitar \t\t Score: 0.0363\n",
      "A man is playing guitar \t\t Do you like pizza? \t\t Score: 0.0116\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The cat sits outside\",\n",
    "    \"A man is playing guitar\",\n",
    "    \"I love pasta\",\n",
    "    \"The new movie is awesome\",\n",
    "    \"The cat plays in the garden\",\n",
    "    \"A woman watches TV\",\n",
    "    \"The new movie is so great\",\n",
    "    \"Do you like pizza?\",\n",
    "]\n",
    "\n",
    "paraphrases = util.paraphrase_mining(sent_model, sentences, top_k=3, batch_size=4)\n",
    "\n",
    "for paraphrase in paraphrases:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a2102-9d84-4448-b08b-f094aae18ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ce5c03-81a1-476a-901a-1ad8be6a4b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 7625.52 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 79732.04 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 78266.54 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 79246.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fs_examp=get_few_shot_examples(dataset['train'])\n",
    "fs_prompt=construct_prompt(fs_examp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0381aaaf-8798-4d67-b7f6-9542848599a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(prompts,gen_config,model_name='aya',bs=8):\n",
    "    all_response=[]\n",
    "    all_response_raw=[]\n",
    "    end=len(prompts)\n",
    "    for start in tqdm(range(0,end,bs)):\n",
    "        stop=min(start+bs,len(prompts))\n",
    "        if start<stop:\n",
    "            prompts_batch=prompts[start:stop]\n",
    "            encodings=tokenizer(prompts_batch, return_tensors=\"pt\", padding='longest', truncation=False).to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(**encodings, **gen_config)\n",
    "            responses=tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for i,response_raw in enumerate(responses):\n",
    "                sample_no=i+start\n",
    "                if model_name!='aya':\n",
    "                    response=response_raw[len(prompts[sample_no]):]\n",
    "                    response=response.split(\"\\n\")[0].strip() if \"\\n\" in response else response.strip()\n",
    "                else:\n",
    "                    response=response_raw[-1]\n",
    "                all_response.append(response)\n",
    "                all_response_raw.append(response_raw)\n",
    "                \n",
    "    return all_response_raw,all_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5a3facee-8090-42c5-b4cb-36324f037060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_sentences(corpus, queries,k=6):\n",
    "    top_k = min(k, len(corpus))\n",
    "    query_embeddings = sent_model.encode(queries, convert_to_tensor=True).to(\"cuda\")\n",
    "    corpus_embeddings = sent_model.encode(corpus, convert_to_tensor=True).to(\"cuda\")\n",
    "    corpus_embeddings = util.normalize_embeddings(corpus_embeddings)\n",
    "    query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "    hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k,\n",
    "                                score_function=util.dot_score)\n",
    "    all_hits={}\n",
    "    for i,hit in enumerate(hits):\n",
    "        all_hits[i]=[]\n",
    "        for j in range(0,top_k,1):\n",
    "            all_hits[i].append(hits[i][j]['corpus_id'])\n",
    "    return all_hits\n",
    "\n",
    "\n",
    "def get_data_simset(train_data,val_data):\n",
    "    new_ds=[]\n",
    "    new_ds_simset={}\n",
    "    count=0\n",
    "    for q in ['cause','effect']:\n",
    "        q_examples_corpus = train_data.filter(lambda example:  \n",
    "                                                 example[\"question\"]==q)\n",
    "        q_examples_query = val_data.filter(lambda example:  \n",
    "                                                 example[\"question\"]==q)\n",
    "        print(len(q_examples_query))\n",
    "        corpus=q_examples_corpus['premise']\n",
    "        query=q_examples_query['premise']\n",
    "        x=get_similar_sentences(corpus,query)\n",
    "        for i,corp in enumerate(q_examples_query):\n",
    "            new_ds.append(q_examples_query[i])\n",
    "            # print(query[i],q_examples_query[i])\n",
    "            new_ds_simset[count]=[]\n",
    "            for j in x[i]:\n",
    "                new_ds_simset[count].append(q_examples_corpus[j])\n",
    "                # print(i,corpus[j],q_examples_corpus[j],end=',')\n",
    "            # print('\\n')\n",
    "            count+=1\n",
    "    return new_ds,new_ds_simset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "db486e65-c027-4f88-b938-d67cf6dff3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 400\n",
      "val size: 100\n"
     ]
    }
   ],
   "source": [
    "gen_config = {\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.1,\n",
    "                \"repetition_penalty\": 1.18,\n",
    "                \"top_k\": 40,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 5,\n",
    "                \"pad_token_id\": tokenizer.eos_token_id\n",
    "                    }\n",
    "\n",
    "dataset=load_datasets(\"copa-en\")\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# all_prompt_examp={'train':[],'val':[]}\n",
    "# all_prompt_labels={'train':[],'val':[]}\n",
    "\n",
    "# for split in ['train','val']:\n",
    "#     for row in dataset[split]:\n",
    "#         fs_examp = get_similar_fewshot()\n",
    "#         fs_prompt=construct_prompt(fs_examp)\n",
    "        # prompt=(fs_prompt + \"\\n\\n\" + prompt_template.format(**row, correct_answer=\"\")).strip()\n",
    "        # all_prompt_examp[split].append(prompt)\n",
    "        # all_prompt_labels[split].append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "fd74323e-a60e-440d-9b25-d8026879ee89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "09faa98a-70b0-4ee0-bd02-5694f15a9b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 144.52ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 522.78ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 196.46ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 545.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "new_ds,new_ds_simset = get_data_simset(dataset['train'],dataset['val'])\n",
    "\n",
    "all_test_prompt=[]\n",
    "all_test_label=[]\n",
    "for i,d in enumerate(new_ds):\n",
    "    # if i==1:\n",
    "    #     break\n",
    "    fs_prompt=test_construct_Prompt(new_ds_simset[i],6)\n",
    "    prompt_ex=construct_single(d,fs_prompt)\n",
    "    all_test_prompt.append(prompt_ex)\n",
    "    all_test_label.append(d['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c9a17e4d-dc41-4d1e-8a23-5c58688f6d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant whose goal is to select the correct output for a given instruction in english.\n",
      "\n",
      "\n",
      "Instruction: Given the premise, \"\"I pushed the pendulum.\"\", What is the correct effect after this?\n",
      "A: It slowed to a stop.\n",
      "B: It swung back and forth.\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"I flipped the light switch up and down.\"\", What is the correct effect after this?\n",
      "A: The light faded.\n",
      "B: The light flickered.\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"I tipped the bottle.\"\", What is the correct effect after this?\n",
      "A: The liquid in the bottle froze.\n",
      "B: The liquid in the bottle poured out.\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"I wanted to lighten the mood of the conversation.\"\", What is the correct effect after this?\n",
      "A: I remained quiet.\n",
      "B: I told a joke.\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"The room was dim.\"\", What is the correct effect after this?\n",
      "A: I opened the blinds.\n",
      "B: I washed the windows.\n",
      "Correct effect: A\n",
      "\n",
      "Instruction: Given the premise, \"\"I pushed the gas pedal.\"\", What is the correct effect after this?\n",
      "A: The car accelerated.\n",
      "B: The car door opened.\n",
      "Correct effect: A\n",
      "\n",
      "Instruction: Given the premise, \"\"I lit the candle.\"\", What is the correct effect after this?\n",
      "A: Wax dripped off the candle.\n",
      "B: The wax on the candle hardened.\n",
      "Correct effect:\n"
     ]
    }
   ],
   "source": [
    "print(all_test_prompt[93])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e3f00e4a-98c8-4f3a-8076-a567f8e21e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2fec567b-fdee-4ea0-aa8e-af2e26c5cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:29<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "all_response_raw,all_response=generate_result(all_test_prompt,\n",
    "                                              gen_config,'aya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1fc84ca1-02a0-4f12-a593-911be6f55ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85 85 100 100 [3, 4, 8, 21, 23, 27, 32, 35, 39, 41, 45, 68, 69, 73, 93] ['B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'B']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(all_response,all_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54cf29a3-717e-4063-b429-c83504e218f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# split='train'\n",
    "# with open('../result/test_result.pickle','wb') as f:\n",
    "#     pickle.dump(all_response,f)\n",
    "\n",
    "# with open('../result/test_label.pickle','wb') as f:\n",
    "#     pickle.dump(all_prompt_labels[split],f)\n",
    "\n",
    "# with open('../result/test_prompt.pickle','wb') as f:\n",
    "#     pickle.dump(all_prompt_examp[split],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55719c37-1a07-46e5-8878-046449e42aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_not_true={}\n",
    "count=0\n",
    "for i,row in enumerate(dataset['train']):\n",
    "    if all_response[i] in choices:\n",
    "        if choices.index(all_response[i])!=all_prompt_labels['train'][i]:\n",
    "            count+=1\n",
    "            all_not_true[i]={\n",
    "                'original':row,\n",
    "                'test_label':all_prompt_labels['train'][i],\n",
    "                'pred_label':choices.index(all_response[i])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab05e5f-cced-4dbd-b7de-e1fc7cf22525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fd987a1-d137-4661-a694-dae738fccdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs_examp=get_few_shot_examples(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "bd97840b-0c56-4579-89e4-8b14381c3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang='english'\n",
    "choices=[\"A\",\"B\"]\n",
    "\n",
    "preamble = f\"\"\"You are a helpful assistant whose goal is to select the correct output for a given instruction in {lang}.\"\"\"\n",
    "\n",
    "# preamble =\"\"\"\"\"\"\n",
    "# ---46%\n",
    "# prompt_template_cause=\"\"\"Instruction: Given the premise, \"\"{premise}\"\", What is the correct {question}?\n",
    "# {question} A: {choice1}\n",
    "# {question} B: {choice2}\n",
    "# Correct {question}: {correct_answer}\"\"\"\n",
    "\n",
    "# prompt_template_effect=\"\"\"Instruction: Given the premise, \"\"{premise}\"\", What is the correct {question}?\n",
    "# {question} A: {choice1}\n",
    "# {question} B: {choice2}\n",
    "# Correct {question}: {correct_answer}\"\"\"\n",
    "\n",
    "# ---51%\n",
    "prompt_template_cause=\"\"\"Instruction: Given the premise, \"\"{premise}\"\", What is the correct {question} before this?\n",
    "A: {choice1}\n",
    "B: {choice2}\n",
    "Correct {question}: {correct_answer}\"\"\"\n",
    "\n",
    "prompt_template_effect=\"\"\"Instruction: Given the premise, \"\"{premise}\"\", What is the correct {question} after this?\n",
    "A: {choice1}\n",
    "B: {choice2}\n",
    "Correct {question}: {correct_answer}\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ff5fcb14-4045-445b-aab4-413ee3ca13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_examples(dataset, question,fs_per_label=2, seed=42):\n",
    "    labels = list(set(dataset[\"label\"]))\n",
    "    few_shot_examples = []\n",
    "    for label in labels:\n",
    "        label_examples = dataset.filter(lambda example: example[\"label\"] == label and example[\"question\"]==question)\n",
    "        # shuffle the examples\n",
    "        label_examples = label_examples.shuffle(seed=seed)\n",
    "        # get the first fs_per_label examples\n",
    "        label_examples = label_examples.select(\n",
    "            range(min(fs_per_label, len(label_examples)))\n",
    "        )\n",
    "        few_shot_examples += [example for example in label_examples]\n",
    "\n",
    "    # Shuffle the few shot examples\n",
    "    random.shuffle(few_shot_examples)\n",
    "    return few_shot_examples\n",
    "\n",
    "def test_construct_Prompt(ds_examples,min_ex=2):\n",
    "    ds_examples=ds_examples[:min_ex]\n",
    "    prompt_examples = \"\\n\\n\".join([ prompt_template_cause.format(**d,correct_answer=choices[int(d[\"label\"])]) \n",
    "                                   if d[\"question\"]=='cause' \n",
    "                                   else prompt_template_effect.format(**d,correct_answer=choices[int(d[\"label\"])])\n",
    "                                   for d in ds_examples])\n",
    "    prompt_examples=preamble+\"\\n\\n\\n\"+prompt_examples\n",
    "    return prompt_examples\n",
    "\n",
    "def construct_single(row,fs_prompt):\n",
    "    if row['question']=='cause':\n",
    "        prompt=(fs_prompt + \"\\n\\n\" + prompt_template_cause.format(**row, correct_answer=\"\")).strip()\n",
    "        # prompt=(prompt_template_cause.format(**row, correct_answer=\"\")).strip()\n",
    "    else:\n",
    "        prompt=(fs_prompt + \"\\n\\n\" + prompt_template_effect.format(**row, correct_answer=\"\")).strip()\n",
    "        # prompt=( prompt_template_cause.format(**row, correct_answer=\"\")).strip()\n",
    "    return prompt\n",
    "\n",
    "def eval(all_preds,all_true_labels):\n",
    "    count=0\n",
    "    ind_true=[]\n",
    "    not_true=[]\n",
    "    indx=[]\n",
    "    for i,res in enumerate(all_preds):\n",
    "        if res in choices:\n",
    "            if choices.index(res)==all_true_labels[i]:\n",
    "                count+=1\n",
    "                ind_true.append(i)\n",
    "            else:\n",
    "                not_true.append(i)\n",
    "                indx.append(res)\n",
    "    acc=count/len(all_preds)\n",
    "    print(acc, count, len(all_preds), len(all_true_labels), not_true,indx)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "595ff062-cd73-4282-a445-5dd629eeb4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['effect B', 'cause B', 'cause B', 'cause A', 'effect B', 'effect B', 'cause A', 'effect B', 'cause B', 'effect B', 'effect A', 'cause A', 'cause B', 'cause B', 'cause B', 'cause B', 'effect B', 'effect A', 'effect B', 'effect B', 'cause A', 'effect B', 'cause A', 'effect B', 'cause B', 'cause B', 'effect A', 'effect B', 'effect A', 'cause A', 'cause B', 'cause B', 'cause A', 'effect B', 'cause B', 'cause A', 'cause A', 'cause A', 'cause B', 'cause B', 'effect A', 'cause A', 'cause A', 'cause A', 'cause B', 'cause A', 'effect B', 'effect B: \"\"The', 'cause A', 'cause B', 'effect B', 'effect A', 'cause B', 'cause B', 'effect B']\n"
     ]
    }
   ],
   "source": [
    "print(all_response_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19626710-0805-4a6f-9fed-892540b48767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 68152.97 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 77417.82 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 75556.03 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 80489.43 examples/s]\n",
      "100%|██████████| 7/7 [00:12<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2909090909090909 16 55 55 [0, 4, 17, 22, 23, 24, 28, 29, 30, 41, 42, 43, 45, 48, 52, 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 66809.56 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 79509.10 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 79773.74 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 77475.02 examples/s]\n",
      "100%|██████████| 7/7 [00:12<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 22 55 55 [0, 4, 7, 9, 18, 22, 24, 26, 28, 29, 30, 31, 35, 41, 42, 43, 45, 48, 50, 52, 53, 54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 66639.72 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 73497.24 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 80358.35 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 80975.03 examples/s]\n",
      "100%|██████████| 7/7 [00:12<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41818181818181815 23 55 55 [0, 7, 9, 18, 19, 22, 24, 26, 28, 29, 30, 31, 35, 41, 42, 43, 45, 48, 50, 51, 52, 53, 54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 55847.73 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 80051.61 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 78055.35 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 77539.47 examples/s]\n",
      "100%|██████████| 7/7 [00:12<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38181818181818183 21 55 55 [0, 7, 9, 17, 18, 22, 23, 24, 26, 28, 29, 30, 41, 42, 43, 45, 48, 50, 52, 53, 54]\n",
      "[0.2909090909090909, 0.4, 0.41818181818181815, 0.38181818181818183]\n",
      "0.37272727272727274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_acc=[]\n",
    "\n",
    "\n",
    "for ii in range(0,4,1):\n",
    "    all_test_prompt=[]\n",
    "    all_test_label=[]\n",
    "    fs_examp_cause=get_few_shot_examples(dataset['train'],'cause',fs_per_label=2,seed=ii)\n",
    "    fs_examp_effect=get_few_shot_examples(dataset['train'],'effect',fs_per_label=2,seed=ii)\n",
    "    fs_prompt_cause=test_construct_Prompt(fs_examp_cause)\n",
    "    fs_prompt_effect=test_construct_Prompt(fs_examp_effect)\n",
    "\n",
    "    count=0\n",
    "    for i,row in all_not_true.items():\n",
    "        if row['original']['question']=='casue':\n",
    "            fs_prompt=fs_prompt_cause\n",
    "        else:\n",
    "            fs_prompt=fs_prompt_effect\n",
    "        prompt_ex=construct_single(row['original'],fs_prompt)\n",
    "        all_test_prompt.append(prompt_ex)\n",
    "        all_test_label.append(row['original']['label'])\n",
    "        count+=1\n",
    "    all_response_raw_test,all_response_test=generate_result(all_test_prompt,\n",
    "                                                  gen_config,'aya')\n",
    "\n",
    "    acc=eval(all_response_test,all_test_label)\n",
    "    all_acc.append(acc)\n",
    "print(all_acc)\n",
    "print(sum(all_acc)/len(all_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51e672f6-329f-4780-aa68-82176d08985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ct effect:\n",
      "ct effect:\n",
      "ct effect:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ct effect:\n",
      "ct effect:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ct effect:\n",
      "ect cause:\n",
      "ect cause:\n",
      "ct effect:\n"
     ]
    }
   ],
   "source": [
    "all_c=[0, 9, 10, 22, 24, 26, 28, 29, 30, 41, 42, 43, 45, 48, 50, 52, 53, 54]\n",
    "for i in all_c:\n",
    "    print(all_test_prompt[i][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66c852c7-dc75-4b66-b972-5f3800a285ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant whose goal is to select the correct output for a given instruction in english.\n",
      "\n",
      "\n",
      "Instruction: Given the premise, \"\"The host served dinner to his guests.\"\", What is the correct effect after this?\n",
      "effect A: \"\"His guests were gracious.\"\"\n",
      "effect B: \"\"His guests went hungry.\"\"\n",
      "Correct effect: A\n",
      "\n",
      "Instruction: Given the premise, \"\"My foot went numb.\"\", What is the correct effect after this?\n",
      "effect A: \"\"I put my shoes on.\"\"\n",
      "effect B: \"\"I shook my foot.\"\"\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"The teacher caught the student chewing gum.\"\", What is the correct effect after this?\n",
      "effect A: \"\"The gum stuck to the student's shoe.\"\"\n",
      "effect B: \"\"The student spit out the gum.\"\"\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"The man lifted the heavy box.\"\", What is the correct effect after this?\n",
      "effect A: \"\"He put out his back.\"\"\n",
      "effect B: \"\"He scratched his back.\"\"\n",
      "Correct effect: A\n",
      "\n",
      "Instruction: Given the premise, \"\"The elderly woman suffered a stroke.\"\", What is the correct effect after this?\n",
      "effect A: \"\"The woman's daughter came over to clean her house.\"\"\n",
      "effect B: \"\"The woman's daughter moved in to take care of her.\"\"\n",
      "Correct effect: 1 B\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(all_test_prompt[i],all_test_label[i],all_response_test[i],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e3f62-a075-42e1-8de3-dce69be50512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005d6a73-c2b7-43ba-821b-a22cd91f3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict,concatenate_datasets\n",
    "from utility import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aca3e663-62d8-4eda-a4aa-fd5f10d15cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_fs_general(dataset, count=4,seed=42):\n",
    "\n",
    "    fs_examps=[]\n",
    "    for q in ['cause','effect']:\n",
    "        data_q = train_data.filter(lambda example:  \n",
    "                                                 example[\"question\"]==q)\n",
    "        for i in range(0,len(data_q),1):\n",
    "            start=i\n",
    "            end=i+count\n",
    "            if end>len(data_q):\n",
    "                borrow=end-len(data_q)\n",
    "                indices=list(range(start,len(data_q)))+list(range(0,borrow))\n",
    "            else:\n",
    "                indices=list(range(start,end))\n",
    "            fs_examps.append(dataset.select(indices))\n",
    "    return fs_examps  \n",
    "\n",
    "def get_similar_sentences(corpus, queries,k=4):\n",
    "    top_k = min(k, len(corpus))\n",
    "    query_embeddings = sent_model.encode(queries, convert_to_tensor=True).to(\"cuda\")\n",
    "    corpus_embeddings = sent_model.encode(corpus, convert_to_tensor=True).to(\"cuda\")\n",
    "    corpus_embeddings = util.normalize_embeddings(corpus_embeddings)\n",
    "    query_embeddings = util.normalize_embeddings(query_embeddings)\n",
    "    hits = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k,\n",
    "                                score_function=util.dot_score)\n",
    "    all_hits={}\n",
    "    for i,hit in enumerate(hits):\n",
    "        all_hits[i]=[]\n",
    "        for j in range(0,top_k,1):\n",
    "            all_hits[i].append(hits[i][j]['corpus_id'])\n",
    "    return all_hits\n",
    "\n",
    "def get_all_fs_similar(train_data):\n",
    "    fs_examps=[]\n",
    "    count=0\n",
    "    for q in ['cause','effect']:\n",
    "        q_examples_corpus = train_data.filter(lambda example:  \n",
    "                                                 example[\"question\"]==q)\n",
    "        q_examples_query = train_data.filter(lambda example:  \n",
    "                                                 example[\"question\"]==q)\n",
    "        print(len(q_examples_query))\n",
    "        corpus=q_examples_corpus['premise']\n",
    "        query=q_examples_query['premise']\n",
    "        x=get_similar_sentences(corpus,query)\n",
    "        for i,indices in x.items():\n",
    "            fs_examps.append(q_examples_corpus.select(indices))\n",
    "    return fs_examps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32204222-277b-4ac9-b089-2c7a5834db58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n",
      "train size: 400\n",
      "val size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 188.20ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 206.33ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 162.35ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 201.79ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 204.38ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 208.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "from get_prompts import *\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Hello, World!\")\n",
    "    \n",
    "    \n",
    "    dataset=load_datasets(\"copa-en\")\n",
    "    train_data=dataset['train']\n",
    "    fs_examps_general=get_all_fs_general(train_data)\n",
    "    fs_prompts_general=[construct_prompt_general(x) for x in fs_examps_general]\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    sent_model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(\"cuda\")\n",
    "    fs_examps_similar=get_all_fs_similar(train_data)\n",
    "    fs_prompts_general=[construct_prompt_general(x) for x in fs_examps_similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5bcf3d22-83c1-42f0-bcca-643d279b408b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:32<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "gen_config = {\n",
    "                \"temperature\": 0.9,\n",
    "                # \"top_p\": 0.1,\n",
    "                # \"repetition_penalty\": 1.18,\n",
    "                \"top_k\": 20,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 2062,\n",
    "                \"pad_token_id\": tokenizer.eos_token_id\n",
    "                    }\n",
    "all_response_raw,all_response=generate_result(fs_prompts_general,\n",
    "                                              gen_config,'aya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b05dc0c0-cfe2-45cd-bf7a-8995a8c14d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_formated=[]\n",
    "for i,response in enumerate(all_response_raw):\n",
    "    if not response.startswith('\"correct'):\n",
    "        premise=response.split(' \"correct')[0]\n",
    "        if \"correct cause\" in response:\n",
    "            q=\"cause\"\n",
    "        elif \"correct effect\" in response:\n",
    "            q=\"effect\"\n",
    "        if len(response.split('\": '))==1:\n",
    "            continue\n",
    "        correct_q=response.split('\": ')[1].split(' \"')[0]\n",
    "        wrong_q=response.split('\": ')[-1]\n",
    "        if i%2==0:\n",
    "            label=0\n",
    "            x={\n",
    "            \"premise\":premise,\n",
    "            \"choice1\":correct_q,\n",
    "            \"choice2\":wrong_q,\n",
    "            \"question\":q,\n",
    "            \"label\":label,\n",
    "            \"idx\":i\n",
    "        }\n",
    "        else:\n",
    "            label=1\n",
    "            x={\n",
    "            \"premise\":premise,\n",
    "            \"choice1\":wrong_q,\n",
    "            \"choice2\":correct_q,\n",
    "            \"question\":q,\n",
    "            \"label\":label,\n",
    "            \"idx\":i\n",
    "        }\n",
    "\n",
    "\n",
    "        all_formated.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dbd726b-228c-4275-8f00-1dd90c7655cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dest_file=\"../data/synthetic/copa-en-train-similar.jsonl\"\n",
    "output_file = open(dest_file, 'w', encoding='utf-8')\n",
    "for dic in all_formated:\n",
    "    json.dump(dic, output_file) \n",
    "    output_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ffc008d1-4b70-455d-8cba-1ed66f635208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_formated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b2c67-3b0e-4cbc-809d-c7821a8da4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copa",
   "language": "python",
   "name": "copa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
