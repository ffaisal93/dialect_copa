{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20f598d2-f62f-4230-9932-8bad4bc7b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import logging\n",
    "from generate_utility import *\n",
    "from transformers.generation import GenerationConfig\n",
    "from peft import PeftModel\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# CACHE_DIR = \"/gscratch/argon/kahuja/.cache/\"\n",
    "# DATA_DIR = \"data/sentiment_analysis/arabic/\"\n",
    "# MODEL2HFSTR = {\"mistral\": \"mistralai/Mistral-7B-v0.1\"}\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(lang):\n",
    "    datasets = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        with open(os.path.join(DATADIR,lang,f\"{split}.jsonl\"), encoding=\"utf-8\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "        datasets[split] = pd.DataFrame(line_dicts)\n",
    "        datasets[split] = Dataset.from_pandas(datasets[split])\n",
    "        # datasets[split] = datasets[split].filter(\n",
    "        #     lambda example: not (\n",
    "        #         example[\"text\"] == \"sentence\" and example[\"label\"] == \"label\"\n",
    "        #     )\n",
    "        # )\n",
    "        print(f\"{split} size: {len(datasets[split])}\")\n",
    "\n",
    "    datasets = DatasetDict(datasets)\n",
    "    return datasets\n",
    "\n",
    "    return datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d00d89aa-884b-4925-882a-d99144a3e078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en train.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 4/4 [00:00<00:00, 68.02ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en train.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 162.00ba/s]\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 142.05ba/s]\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 139.54ba/s]\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 153.03ba/s]\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 152.45ba/s]\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 140.20ba/s]\n",
      "Flattening the indices: 100%|██████████| 1/1 [00:00<00:00, 139.06ba/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "DATADIR='../data'\n",
    "def make_all_train_data(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train'):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "def make_all_val_data(DATADIR):\n",
    "    datasets={}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train'):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets[str(f)]=line_dicts\n",
    "                    datasets[str(f)] = Dataset.from_pandas(pd.DataFrame(data=datasets[str(f)]))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "train_datasets=make_all_train_data(DATADIR)\n",
    "train_datasets.save_to_disk(os.path.join(DATADIR,'all_train'))\n",
    "\n",
    "val_datasets=make_all_val_data(DATADIR)\n",
    "val_datasets.save_to_disk(os.path.join(DATADIR,'all_val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05079027-52c8-41dd-81bc-225497fd1c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_datasets['copa-en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6eafb1e-5c5a-43af-b86a-f35e5e757466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_from_disk(os.path.join(DATADIR,'all_train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f44ec42e-adbf-4f11-aebe-c072d7800d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation']=val_datasets['copa-en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "656e2f15-bbfe-4e8e-ac14-2afab0a6fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-hr Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-mk Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sl Dataset({\n",
      "    features: ['choice1', 'choice2', 'idx', 'label', 'premise', 'question'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sl-cer Dataset({\n",
      "    features: ['choice1', 'choice2', 'idx', 'label', 'premise', 'question'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sr Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sr-tor Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for r in val_datasets:\n",
    "    print(r,val_datasets[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96f306ad-62ec-403f-82a7-79cb66a7e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR=\"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56887e71-bd0d-453a-8bd6-275e022ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 400\n",
      "val size: 100\n"
     ]
    }
   ],
   "source": [
    "dataset=load_datasets(\"copa-hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "687f4176-254f-4602-a88e-763963f5c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# lines = []\n",
    "\n",
    "# with open(os.path.join(DATADIR,'copa-hr','train.jsonl')) as f:\n",
    "#     lines = f.read().splitlines()\n",
    "\n",
    "# line_dicts = [json.loads(line) for line in lines]\n",
    "# df_final = pd.DataFrame(line_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1188f51-d21c-43ab-8c90-61e65100f81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e53aac47-3214-4a3a-9d14-693cfff68f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"Croatian\"\n",
    "preamble = f\"\"\"You are a helpful following assistant whose goal is to select the preferred (least wrong) output for a given instruction in {lang}.\"\"\"\n",
    "\n",
    "prompt_template=\"\"\"Instruction: Given the premise, \"\"{premise}\"\", What is the correct {question}?\n",
    "{question} A: {choice1}\n",
    "{question} B: {choice2}\n",
    "Correct {question}: {correct_answer}\"\"\"\n",
    "\n",
    "choices=[\"A\",\"B\"]\n",
    "\n",
    "def get_few_shot_examples(dataset, fs_per_label=1, seed=42):\n",
    "    labels = list(set(dataset[\"label\"]))\n",
    "    few_shot_examples = []\n",
    "    for label in labels:\n",
    "        label_examples = dataset.filter(lambda example: example[\"label\"] == label and example[\"question\"]=='cause')\n",
    "        # shuffle the examples\n",
    "        label_examples = label_examples.shuffle(seed=seed)\n",
    "        # get the first fs_per_label examples\n",
    "        label_examples = label_examples.select(\n",
    "            range(min(fs_per_label, len(label_examples)))\n",
    "        )\n",
    "        few_shot_examples += [example for example in label_examples]\n",
    "        \n",
    "        label_examples = dataset.filter(lambda example: example[\"label\"] == label and example[\"question\"]=='effect')\n",
    "        # shuffle the examples\n",
    "        label_examples = label_examples.shuffle(seed=seed)\n",
    "        # get the first fs_per_label examples\n",
    "        label_examples = label_examples.select(\n",
    "            range(min(fs_per_label, len(label_examples)))\n",
    "        )\n",
    "        few_shot_examples += [example for example in label_examples]\n",
    "\n",
    "    # Shuffle the few shot examples\n",
    "    random.shuffle(few_shot_examples)\n",
    "    return few_shot_examples\n",
    "\n",
    "def construct_prompt(ds_examples):\n",
    "    def example_to_prompt(example, add_label=True):\n",
    "        ex_prompt = f\"Sentence: {example['text']}\\n\"\n",
    "        if add_label:\n",
    "            ex_prompt += f\"Label: {example['label']}\\n\"\n",
    "        return ex_prompt\n",
    "\n",
    "    # To Do: Add domain of the text in the instruction like \"In this task you given text from {domain}\n",
    "\n",
    "    # Format the first five rows as examples for 5-shot prompting\n",
    "    prompt_examples = \"\\n\\n\".join([ prompt_template.format(**d,correct_answer=choices[int(d[\"label\"])-1]) for d in ds_examples])\n",
    "    prompt_examples=preamble+\"\\n\\n\\n\"+prompt_examples\n",
    "    return prompt_examples\n",
    "#     for example in fs_examples:\n",
    "#         prompt += example_to_prompt(example, add_label=True)\n",
    "#         prompt += \"\\n\"\n",
    "\n",
    "#     if not prompt_for_each_label:\n",
    "#         prompt += example_to_prompt(test_example, add_label=False)\n",
    "#         return prompt\n",
    "#     else:\n",
    "#         prompts = [\n",
    "#             prompt + example_to_prompt(test_example, add_label=True) for label in labels\n",
    "#         ]\n",
    "#         gold_label_idx = labels.index(test_example[\"label\"])\n",
    "#         return prompts, gold_label_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2529b2d6-5138-4554-9dfc-67eb3b10abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 62826.60 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 88506.10 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 88124.89 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 85702.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fs_examp=get_few_shot_examples(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8cab222-be9a-4704-b107-9cd1909bfba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_prompt=construct_prompt(fs_examp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fab0d424-6440-4ca0-b1f0-6fdb2a00657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful following assistant whose goal is to select the preferred (least wrong) output for a given instruction in Croatian.\n",
      "\n",
      "\n",
      "Instruction: Given the premise, \"\"Ometen sam u razgovoru sa ženom.\"\", What is the correct cause?\n",
      "cause A: Svi su u sobi govorili.\n",
      "cause B: Žena je pričala smiješnu priču.\n",
      "Correct cause: B\n",
      "\n",
      "Instruction: Given the premise, \"\"Mačka je prela.\"\", What is the correct cause?\n",
      "cause A: Ogrebla me.\n",
      "cause B: Pomazio sam je.\n",
      "Correct cause: A\n",
      "\n",
      "Instruction: Given the premise, \"\"Ravnatelj škole uveo je pravila oblačenja.\"\", What is the correct effect?\n",
      "effect A: Učenici su se pobunili protiv te odluke.\n",
      "effect B: Učenici su izbačeni iz škole.\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"Starija žena doživjela je moždani udar.\"\", What is the correct effect?\n",
      "effect A: Ženina kći došla joj je očistiti kuću.\n",
      "effect B: Ženina kći uselila se da bi vodila brigu o njoj.\n",
      "Correct effect: A\n"
     ]
    }
   ],
   "source": [
    "print(fs_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53eb3b6-6da5-4ef3-bb86-3fc44459aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.01s/it]\n",
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n",
    "                                                  token=\"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\", \n",
    "                                                 cache_dir='../models/Llama-2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9d5e00-9bfc-48fe-b6f0-4c742ddea37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../models/mala-500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9cca3c4-8935-4a40-8f97-9fabd5a5a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.resize_token_embeddings(260164)\n",
    "model = PeftModel.from_pretrained(base_model, '../models/mala-500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5332f90-c091-4c96-a6ce-f995277fe679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(260164, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(260164, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=260164, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=260164, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd3757d-d676-479d-b024-1d49513b290b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "816d807e-8ce4-4c8e-9441-06d15f91130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 21.50s/it]\n",
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-model', '--model', type=str, help='data folder location', required=True)\n",
    "    args = parser.parse_args()\n",
    "    return args  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # args = get_arguments()\n",
    "    # print(args)\n",
    "    # model_name=args.model\n",
    "    model_name=\"llama2_7b_chat\"\n",
    "    \n",
    "    model_configs={\n",
    "    'llama2_7b_chat':{\n",
    "        'model_path':'/projects/antonis/models/LLaMA-v2/Llama2-chat-hf/7B'\n",
    "    },\n",
    "    'llama2_7b':{\n",
    "        'model_path':'../models/Llama-2-7b'\n",
    "    },\n",
    "    }\n",
    "\n",
    "    load_4bit=False\n",
    "    load_8bit=False\n",
    "\n",
    "\n",
    "    model_path=model_configs[model_name]['model_path']\n",
    "    model, tokenizer = get_model(model_path,load_4bit=False,load_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fd9d8d-7129-4ded-a0af-d7f68a4b7a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:55<00:00,  5.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"../models/aya-101\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3e76a8c-7ce5-4b3e-983a-37689c6ace00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a0d138-43b9-481e-9e48-4769abc4683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'B', 'A', 'B', 'cause B', 'A', 'cause A', 'cause A', 'cause B', 'A']\n"
     ]
    }
   ],
   "source": [
    "encodings=tokenizer(all_val_prompts[0:10], return_tensors=\"pt\", padding='longest', truncation=False).to(\"cuda\")\n",
    "output_ids=model.generate(**encodings, **gen_config)\n",
    "responses=tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f03726-21fc-4a25-b7e1-d8ab20b53cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.1,\n",
    "                \"repetition_penalty\": 1.18,\n",
    "                \"top_k\": 40,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 5,\n",
    "                \"pad_token_id\": tokenizer.eos_token_id\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92d8143e-4bc2-43b0-8e33-82146cc6ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(prompts,gen_config,model_name='aya',bs=4):\n",
    "    all_response=[]\n",
    "    all_response_raw=[]\n",
    "    end=len(prompts)\n",
    "    for start in tqdm(range(0,end,bs)):\n",
    "        stop=min(start+bs,len(prompts)-1)\n",
    "        if start<stop:\n",
    "            prompts_batch=prompts[start:stop]\n",
    "            encodings=tokenizer(prompts_batch, return_tensors=\"pt\", padding='longest', truncation=False).to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(**encodings, **gen_config)\n",
    "            responses=tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for i,response_raw in enumerate(responses):\n",
    "                sample_no=i+start\n",
    "                if model_name!='aya':\n",
    "                    response=response_raw[len(prompts[sample_no]):]\n",
    "                    response=response.split(\"\\n\")[0].strip() if \"\\n\" in response else response.strip()\n",
    "                else:\n",
    "                    response=response_raw[-1]\n",
    "                all_response.append(response)\n",
    "                all_response_raw.append(response_raw)\n",
    "                \n",
    "    return all_response_raw,all_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82686399-36be-4131-adb8-8672e48d8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_prompts=[]\n",
    "all_val_labels=[]\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "for row in dataset['val']:\n",
    "    prompt=(fs_prompt + \"\\n\\n\" + prompt_template.format(**row, correct_answer=\"\")).strip()\n",
    "    all_val_prompts.append(prompt)\n",
    "    all_val_labels.append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd79401e-7afb-4d80-94fb-2edc80783c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:28<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "all_response_raw,all_response=generate_result(all_val_prompts,gen_config,'aya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fd8991f-6833-4c2a-bbce-fc2aa78f0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7474747474747475\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i,res in enumerate(all_response):\n",
    "    if res in choices:\n",
    "        if choices.index(res)==all_val_labels[i]:\n",
    "            count+=1\n",
    "acc=count/len(all_response)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f450a8-f471-4020-8f4c-45b3e184b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13355c4e-0c47-410d-b65a-a2156fc09315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, PreTrainedTokenizerBase\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bf6ec0fe-4f68-49c3-84bb-2b6a6479731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 400\n",
      "val size: 100\n"
     ]
    }
   ],
   "source": [
    "datasets=load_datasets(\"copa-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5c3059c7-474c-48e1-a4fa-e1b0125dd963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'My body cast a shadow over the grass.',\n",
       " 'choice1': 'The sun was rising.',\n",
       " 'choice2': 'The grass was cut.',\n",
       " 'question': 'cause',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5834547a-ba91-4a94-91f9-e3d2324f064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"classla/bcms-bertic\"\n",
    "config = AutoConfig.from_pretrained(model_name\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name\n",
    "    )\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1c141feb-6435-4a08-8971-57825d53b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_name = \"premise\"\n",
    "question_header_name = \"question\"\n",
    "ending_names = [f\"choice{i}\" for i in [1, 2]]\n",
    "num_answers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "73aeaece-8f54-4b5b-84f6-cad1df28b4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Map: 100%|██████████| 400/400 [00:00<00:00, 7589.58 examples/s]\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6192.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [\n",
    "      [f\"{context} What was the {question}?\"] * num_answers for context, question in zip(examples[context_name], examples[question_header_name])\n",
    "    ]\n",
    "    second_sentences = [\n",
    "            [f\"{examples[end][i]}\" for end in ending_names] for i, _ in enumerate(first_sentences)\n",
    "        ]\n",
    "    # Flatten out\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Un-flatten\n",
    "    # unflattened = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "    unflattened = {k: [v[i : i + num_answers] for i in range(0, len(v), num_answers)] for k, v in tokenized_examples.items()}\n",
    "    return unflattened\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "preprocess_function,\n",
    "batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fee736a2-9aa4-4ec3-b9c1-87ba8da878d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = (\n",
    "    DataCollatorForMultipleChoice(tokenizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "58f692ad-2e0f-47d1-9bf5-373c79400c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_predictions):\n",
    "        predictions, label_ids = eval_predictions\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "55881a6d-f622-4ee6-9c85-d2b5f4e31586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "136d49b8-77d7-4c7a-903d-65900aaa8cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "66f8db4c-ed11-4057-817e-62c044004920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13.1622, 'train_samples_per_second': 91.17, 'train_steps_per_second': 11.396, 'train_loss': 0.6914518229166666, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7027a5e2-ba86-488b-9829-910868a38e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6911947727203369, 'eval_accuracy': 0.47999998927116394, 'eval_runtime': 0.3688, 'eval_samples_per_second': 271.154, 'eval_steps_per_second': 35.25, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "esults = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3a03fae0-c29a-4b50-a4ed-86ab9067b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng---\n",
    "# mbert=55\n",
    "# bert=66\n",
    "# xlmr=54\n",
    "# bertic=48\n",
    "# llama2-chat=75\n",
    "# mala-500=50\n",
    "# aya=80\n",
    "\n",
    "#copa-hr\n",
    "# bert=55\n",
    "# mbert=57\n",
    "# xlm-r=54\n",
    "# bertic=64\n",
    "# llama2 = 50\n",
    "# aya= 75\n",
    "\n",
    "##-modeling from scratch/finetuning with llm-augmented data generation?\n",
    "# improve dialectal performance with the aid of llm\n",
    "# trend\n",
    "# - previously: llm works or not, benchmarking\n",
    "# - current: reinventing mbert experiments: adding augmented data to improve the performance, llm generated data to train\n",
    "# - next: Think outside of the traditional benchmarking?\n",
    "# -     taking high resource and it's low-resource counterparts. \n",
    "        #Use prompt techniques \n",
    "        #See results and use the insight from there to use lora training. \n",
    "# - low-resource data is the bottolneck here. how much augmentation/synthetic data actually help. In which cases, it must fail?\n",
    "# - what extra thing can llm does which mbert cant do except synthetic data generation\n",
    "\n",
    "\n",
    "1. ft on all other data at once/one by one (mbert, xlm-r, bertic)\n",
    "2. get result from aya on all of them (2 cause, 4 cause in prompt)\n",
    "3. for a give test example, try to choose most similar training examples, in prompt provide info about dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6ca25-8b41-47ec-a38e-d62edc4134db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copa",
   "language": "python",
   "name": "copa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
