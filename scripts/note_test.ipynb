{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f598d2-f62f-4230-9932-8bad4bc7b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk, concatenate_datasets\n",
    "from transformers import logging\n",
    "from generate_utility import *\n",
    "from transformers.generation import GenerationConfig\n",
    "from peft import PeftModel\n",
    "import datasets\n",
    "# import bitsandbytes as bnb\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# CACHE_DIR = \"/gscratch/argon/kahuja/.cache/\"\n",
    "# DATA_DIR = \"data/sentiment_analysis/arabic/\"\n",
    "# MODEL2HFSTR = {\"mistral\": \"mistralai/Mistral-7B-v0.1\"}\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(lang):\n",
    "    datasets = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        with open(os.path.join(DATADIR,lang,f\"{split}.jsonl\"), encoding=\"utf-8\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "        datasets[split] = pd.DataFrame(line_dicts)\n",
    "        datasets[split] = Dataset.from_pandas(datasets[split])\n",
    "        # datasets[split] = datasets[split].filter(\n",
    "        #     lambda example: not (\n",
    "        #         example[\"text\"] == \"sentence\" and example[\"label\"] == \"label\"\n",
    "        #     )\n",
    "        # )\n",
    "        print(f\"{split} size: {len(datasets[split])}\")\n",
    "\n",
    "    datasets = DatasetDict(datasets)\n",
    "    return datasets\n",
    "\n",
    "    return datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d00d89aa-884b-4925-882a-d99144a3e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "DATADIR='../data'\n",
    "def make_all_train_data(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train') and 'reverse' not in str(f1) and 'genx' not in str(f1):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def make_all_train_data_select(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f) in ['copa-ck','copa-hr','copa-sl','copa-sl-cer','copa-sr','copa-sr-tor','copa-ck','copa-mk']:\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train') and ('reverse' not in str(f1)) and ('genx' not in str(f1)):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def make_all_train_data_wrev(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train') and 'genx' not in str(f1):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "def make_all_train_data_wrev_select(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f) in ['copa-ck','copa-hr','copa-sl','copa-sl-cer','copa-sr','copa-sr-tor','copa-ck','copa-mk']:\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train') and 'genx' not in str(f1):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "def make_all_train_data_wrev_wgenx(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train') :\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    for i,line in enumerate(line_dicts):\n",
    "                        line_dicts[i]['idx']=str(line_dicts[i]['idx'])\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "def make_all_train_data_wrev_wgenx_select(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f) in ['copa-ck','copa-hr','copa-sl','copa-sl-cer','copa-sr','copa-sr-tor','copa-ck','copa-mk']:\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train') :\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    for i,line in enumerate(line_dicts):\n",
    "                        line_dicts[i]['idx']=str(line_dicts[i]['idx'])\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "def make_all_train_data_synthetic(DATADIR):\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train'):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('synthetic'):\n",
    "            print(f)\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('copa') and 'train' in str(f1):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets['train'].extend(line_dicts)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def make_all_val_data(DATADIR):\n",
    "    datasets={}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('val'):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets[str(f)]=line_dicts\n",
    "                    datasets[str(f)] = Dataset.from_pandas(pd.DataFrame(data=datasets[str(f)]))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    # datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "def make_all_test_data(DATADIR):\n",
    "    datasets={}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('test'):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    datasets[str(f)]=line_dicts\n",
    "                    datasets[str(f)] = Dataset.from_pandas(pd.DataFrame(data=datasets[str(f)]))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    # datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "# train_datasets=make_all_train_data(DATADIR)\n",
    "# train_datasets.save_to_disk(os.path.join(DATADIR,'all_train'))\n",
    "\n",
    "# val_datasets=make_all_val_data(DATADIR)\n",
    "# val_datasets.save_to_disk(os.path.join(DATADIR,'all_val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d7e2318-edd6-40b3-91ae-830daeec6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def make_mixmatch(DATADIR):\n",
    "    train_splits=['train','train-genx','train-reverse','train.trans']\n",
    "    all_dirs=[]\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            all_dirs.append(str(f))\n",
    "    all_lines=[]        \n",
    "    for i,dir in enumerate(all_dirs):\n",
    "        for split in train_splits:\n",
    "            other_dirs=all_dirs[:i]+all_dirs[i+1:]\n",
    "            random.shuffle(other_dirs)\n",
    "            if split+'.jsonl' in os.listdir(os.path.join(DATADIR,dir)):\n",
    "                for odir in other_dirs:\n",
    "                    if split+'.jsonl' in os.listdir(os.path.join(DATADIR,odir)):\n",
    "                        break\n",
    "                other_dirs.remove(odir)\n",
    "                for oodir in other_dirs:\n",
    "                    if split+'.jsonl' in os.listdir(os.path.join(DATADIR,oodir)):\n",
    "                        break\n",
    "                print(dir,split, odir,oodir,other_dirs)\n",
    "                with open(os.path.join(DATADIR,dir,split+'.jsonl'), encoding=\"utf-8\") as f2:\n",
    "                    lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                with open(os.path.join(DATADIR,odir,split+'.jsonl'), encoding=\"utf-8\") as f3:\n",
    "                    olines = f3.read().splitlines()\n",
    "                    oline_dicts = [json.loads(line) for line in olines]\n",
    "                with open(os.path.join(DATADIR,oodir,split+'.jsonl'), encoding=\"utf-8\") as f4:\n",
    "                    oolines = f4.read().splitlines()\n",
    "                    ooline_dicts = [json.loads(line) for line in oolines]\n",
    "                for l1,l2,l3 in zip(line_dicts,oline_dicts,ooline_dicts):\n",
    "                    new_line={\n",
    "                        \"premise\": l1[\"premise\"], \n",
    "                        \"choice1\": l2[\"choice1\"], \n",
    "                        \"choice2\": l3[\"choice2\"], \n",
    "                        \"question\": l1[\"question\"], \n",
    "                        \"label\": l1[\"label\"], \n",
    "                        \"idx\": 'mix-'+str(l1[\"idx\"])\n",
    "                    }\n",
    "                    all_lines.append(new_line)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=all_lines))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets\n",
    "\n",
    "def make_mixmatch_select(DATADIR):\n",
    "    train_splits=['train','train-genx','train-reverse','train.trans']\n",
    "    all_dirs=[]\n",
    "    datasets={'train':[]}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f) in ['copa-ck','copa-hr','copa-sl','copa-sl-cer','copa-sr','copa-sr-tor','copa-ck','copa-mk']:\n",
    "            all_dirs.append(str(f))\n",
    "    all_lines=[]        \n",
    "    for i,dir in enumerate(all_dirs):\n",
    "        for split in train_splits:\n",
    "            other_dirs=all_dirs[:i]+all_dirs[i+1:]\n",
    "            random.shuffle(other_dirs)\n",
    "            if split+'.jsonl' in os.listdir(os.path.join(DATADIR,dir)):\n",
    "                for odir in other_dirs:\n",
    "                    if split+'.jsonl' in os.listdir(os.path.join(DATADIR,odir)):\n",
    "                        break\n",
    "                other_dirs.remove(odir)\n",
    "                for oodir in other_dirs:\n",
    "                    if split+'.jsonl' in os.listdir(os.path.join(DATADIR,oodir)):\n",
    "                        break\n",
    "                print(dir,split, odir,oodir,other_dirs)\n",
    "                with open(os.path.join(DATADIR,dir,split+'.jsonl'), encoding=\"utf-8\") as f2:\n",
    "                    lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                with open(os.path.join(DATADIR,odir,split+'.jsonl'), encoding=\"utf-8\") as f3:\n",
    "                    olines = f3.read().splitlines()\n",
    "                    oline_dicts = [json.loads(line) for line in olines]\n",
    "                with open(os.path.join(DATADIR,oodir,split+'.jsonl'), encoding=\"utf-8\") as f4:\n",
    "                    oolines = f4.read().splitlines()\n",
    "                    ooline_dicts = [json.loads(line) for line in oolines]\n",
    "                for l1,l2,l3 in zip(line_dicts,oline_dicts,ooline_dicts):\n",
    "                    new_line={\n",
    "                        \"premise\": l1[\"premise\"], \n",
    "                        \"choice1\": l2[\"choice1\"], \n",
    "                        \"choice2\": l3[\"choice2\"], \n",
    "                        \"question\": l1[\"question\"], \n",
    "                        \"label\": l1[\"label\"], \n",
    "                        \"idx\": 'mix-'+str(l1[\"idx\"])\n",
    "                    }\n",
    "                    all_lines.append(new_line)\n",
    "    datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=all_lines))\n",
    "    datasets = DatasetDict(datasets)\n",
    "    datasets=datasets.shuffle(seed=41)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20486e-9dba-4e1a-b0d4-5dc19c11aa5f",
   "metadata": {},
   "source": [
    "#### SELECT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8935a82-a17c-4177-a2a4-4c4c66fd1869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-ck train.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 4/4 [00:00<00:00, 67.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label', 'changed'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n",
      "{'premise': 'Dobili sta se na kavi.', 'choice1': 'Kavarna se je znova odprla na novi lokaciji.', 'choice2': 'Hoteli sta pokramljati.', 'question': 'cause', 'idx': 2, 'label': 1, 'changed': None}\n",
      "copa-ck train.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train-reverse.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train-reverse.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train-reverse.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train-reverse.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 6/6 [00:00<00:00, 74.01ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label', 'changed'],\n",
      "        num_rows: 5600\n",
      "    })\n",
      "})\n",
      "{'premise': 'Mažot žaleše.', 'choice1': 'Negovata žena go kritikuvaše.', 'choice2': 'Negovata majka počina.', 'question': 'cause', 'idx': 374, 'label': 1, 'changed': False}\n",
      "copa-ck train-genx.jsonl\n",
      "copa-ck train.jsonl\n",
      "copa-hr train-genx.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train-genx.jsonl\n",
      "copa-sl train-reverse.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train-reverse.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train-genx.jsonl\n",
      "copa-sr train-reverse.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train-reverse.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 21/21 [00:00<00:00, 68.15ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label', 'changed'],\n",
      "        num_rows: 20676\n",
      "    })\n",
      "})\n",
      "{'premise': 'Цвеће у врту је почело да увяде.', 'choice1': 'Било је недостатка падавина.', 'choice2': 'Пеперут је слетео на једно од цвећа.', 'question': 'cause', 'idx': 'gpt4-cause-223', 'label': 0, 'changed': None}\n",
      "copa-ck train copa-mk copa-sl ['copa-sl', 'copa-sl-cer', 'copa-hr', 'copa-sr-tor', 'copa-sr']\n",
      "copa-ck train-genx copa-sl copa-sr ['copa-sl-cer', 'copa-sr', 'copa-hr', 'copa-sr-tor', 'copa-mk']\n",
      "copa-hr train copa-ck copa-sr ['copa-sr', 'copa-mk', 'copa-sl-cer', 'copa-sr-tor', 'copa-sl']\n",
      "copa-hr train-genx copa-sl copa-ck ['copa-ck', 'copa-sl-cer', 'copa-mk', 'copa-sr', 'copa-sr-tor']\n",
      "copa-mk train copa-sl-cer copa-sr-tor ['copa-sr-tor', 'copa-sl', 'copa-hr', 'copa-sr', 'copa-ck']\n",
      "copa-mk train.trans copa-sr-tor copa-sr ['copa-sr', 'copa-sl-cer', 'copa-sl', 'copa-ck', 'copa-hr']\n",
      "copa-sl train copa-ck copa-hr ['copa-hr', 'copa-sr', 'copa-mk', 'copa-sl-cer', 'copa-sr-tor']\n",
      "copa-sl train-genx copa-sr copa-ck ['copa-sl-cer', 'copa-sr-tor', 'copa-ck', 'copa-mk', 'copa-hr']\n",
      "copa-sl train-reverse copa-sl-cer copa-sr ['copa-ck', 'copa-mk', 'copa-sr', 'copa-sr-tor', 'copa-hr']\n",
      "copa-sl-cer train copa-mk copa-sl ['copa-sl', 'copa-sr', 'copa-sr-tor', 'copa-hr', 'copa-ck']\n",
      "copa-sl-cer train-reverse copa-sr copa-sr-tor ['copa-hr', 'copa-sr-tor', 'copa-mk', 'copa-ck', 'copa-sl']\n",
      "copa-sr train copa-sr-tor copa-mk ['copa-mk', 'copa-sl', 'copa-ck', 'copa-sl-cer', 'copa-hr']\n",
      "copa-sr train-genx copa-ck copa-hr ['copa-hr', 'copa-sl-cer', 'copa-sl', 'copa-mk', 'copa-sr-tor']\n",
      "copa-sr train-reverse copa-sl copa-sl-cer ['copa-sl-cer', 'copa-ck', 'copa-hr', 'copa-mk', 'copa-sr-tor']\n",
      "copa-sr train.trans copa-mk copa-sr-tor ['copa-sr-tor', 'copa-hr', 'copa-sl', 'copa-ck', 'copa-sl-cer']\n",
      "copa-sr-tor train copa-hr copa-sr ['copa-sr', 'copa-sl', 'copa-ck', 'copa-sl-cer', 'copa-mk']\n",
      "copa-sr-tor train-reverse copa-sl copa-sl-cer ['copa-hr', 'copa-ck', 'copa-sl-cer', 'copa-mk', 'copa-sr']\n",
      "copa-sr-tor train.trans copa-mk copa-sr ['copa-ck', 'copa-sl-cer', 'copa-hr', 'copa-sl', 'copa-sr']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 21/21 [00:00<00:00, 75.51ba/s]\n",
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/datasets/table.py:1315: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx'],\n",
      "        num_rows: 20676\n",
      "    })\n",
      "})\n",
      "{'premise': 'Атлет је победио у трци.', 'choice1': 'Naporno je trenirao i poboljšao svoj učinak.', 'choice2': 'Išao je u običnu šetnju svako jutro.', 'question': 'cause', 'label': 0, 'idx': 'mix-gpt4-cause-1001'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 42/42 [00:00<00:00, 67.44ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'choice1', 'choice2', 'question', 'idx', 'label', 'changed'],\n",
      "        num_rows: 41352\n",
      "    })\n",
      "})\n",
      "{'premise': 'Pes je izkopal luknjo na dvorišču.', 'choice1': 'Vlasnik je upravo kosio travnjak.', 'choice2': 'Она је сахрањивала своју кост.', 'question': 'cause', 'idx': 'mix-gpt4-cause-1840', 'label': 1, 'changed': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_datasets=make_all_train_data_select(DATADIR)\n",
    "DATADIR='../data'\n",
    "train_datasets.save_to_disk(os.path.join(DATADIR,'all_train_select'))\n",
    "print(train_datasets)\n",
    "print(train_datasets['train'][0])\n",
    "\n",
    "train_datasets=make_all_train_data_wrev_select(DATADIR)\n",
    "DATADIR='../data'\n",
    "train_datasets.save_to_disk(os.path.join(DATADIR,'all_train_rev_select'))\n",
    "print(train_datasets)\n",
    "print(train_datasets['train'][0])\n",
    "\n",
    "train_datasets=make_all_train_data_wrev_wgenx_select(DATADIR)\n",
    "DATADIR='../data'\n",
    "train_datasets.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx_select'))\n",
    "print(train_datasets)\n",
    "print(train_datasets['train'][0])\n",
    "\n",
    "DATADIR='../data'\n",
    "train_datasets=make_mixmatch_select(DATADIR)\n",
    "train_datasets.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx_omixmatch_select'))\n",
    "print(train_datasets)\n",
    "print(train_datasets['train'][0])\n",
    "\n",
    "DATADIR='../data'\n",
    "dataset1=load_from_disk(os.path.join(DATADIR,'all_train_rev_genx_select'))\n",
    "dataset2=load_from_disk(os.path.join(DATADIR,'all_train_rev_genx_omixmatch_select'))\n",
    "dataset=DatasetDict({\"train\": concatenate_datasets([dataset1[\"train\"], dataset2[\"train\"]])})\n",
    "dataset=dataset.shuffle(seed=41)\n",
    "dataset.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx_wmixmatch_select'))\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2615008-5f6b-4a6b-b923-c21903bc3ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491492b-e446-4bc0-8b77-d79737716c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75a36dc3-9016-4fa6-b4c6-a4821e16ab87",
   "metadata": {},
   "source": [
    "### NOT-SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0002fe1-2411-4fa9-b5e4-eb2f92dca003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-ck train copa-hr copa-mk ['copa-mk', 'copa-sl-cer', 'copa-sr-tor', 'copa-sr', 'copa-sl', 'copa-en']\n",
      "copa-ck train-genx copa-sr copa-sl ['copa-mk', 'copa-sl-cer', 'copa-sl', 'copa-hr', 'copa-sr-tor', 'copa-en']\n",
      "copa-en train copa-hr copa-ck ['copa-ck', 'copa-sl-cer', 'copa-sr-tor', 'copa-sr', 'copa-mk', 'copa-sl']\n",
      "copa-en train-genx copa-sr copa-sl ['copa-sl-cer', 'copa-sl', 'copa-mk', 'copa-ck', 'copa-sr-tor', 'copa-hr']\n",
      "copa-hr train copa-sl copa-en ['copa-en', 'copa-sl-cer', 'copa-mk', 'copa-ck', 'copa-sr', 'copa-sr-tor']\n",
      "copa-hr train-genx copa-en copa-sl ['copa-sr-tor', 'copa-sl', 'copa-sr', 'copa-mk', 'copa-sl-cer', 'copa-ck']\n",
      "copa-mk train copa-sl-cer copa-hr ['copa-hr', 'copa-sl', 'copa-sr-tor', 'copa-ck', 'copa-en', 'copa-sr']\n",
      "copa-mk train.trans copa-sr copa-sr-tor ['copa-hr', 'copa-en', 'copa-sl', 'copa-sl-cer', 'copa-sr-tor', 'copa-ck']\n",
      "copa-sl train copa-hr copa-sr ['copa-sr', 'copa-mk', 'copa-sr-tor', 'copa-ck', 'copa-sl-cer', 'copa-en']\n",
      "copa-sl train-genx copa-hr copa-sr ['copa-sl-cer', 'copa-mk', 'copa-sr', 'copa-ck', 'copa-sr-tor', 'copa-en']\n",
      "copa-sl train-reverse copa-sr-tor copa-sr ['copa-sr', 'copa-sl-cer', 'copa-ck', 'copa-en', 'copa-hr', 'copa-mk']\n",
      "copa-sl-cer train copa-en copa-sr-tor ['copa-sr-tor', 'copa-ck', 'copa-sr', 'copa-sl', 'copa-hr', 'copa-mk']\n",
      "copa-sl-cer train-reverse copa-sr-tor copa-sl ['copa-hr', 'copa-sl', 'copa-ck', 'copa-sr', 'copa-mk', 'copa-en']\n",
      "copa-sr train copa-ck copa-mk ['copa-mk', 'copa-hr', 'copa-sl-cer', 'copa-sl', 'copa-en', 'copa-sr-tor']\n",
      "copa-sr train-genx copa-en copa-hr ['copa-sr-tor', 'copa-sl-cer', 'copa-mk', 'copa-hr', 'copa-sl', 'copa-ck']\n",
      "copa-sr train-reverse copa-sl-cer copa-sr-tor ['copa-mk', 'copa-hr', 'copa-ck', 'copa-sr-tor', 'copa-en', 'copa-sl']\n",
      "copa-sr train.trans copa-sr-tor copa-mk ['copa-en', 'copa-hr', 'copa-sl-cer', 'copa-ck', 'copa-mk', 'copa-sl']\n",
      "copa-sr-tor train copa-hr copa-ck ['copa-ck', 'copa-en', 'copa-sl-cer', 'copa-sr', 'copa-mk', 'copa-sl']\n",
      "copa-sr-tor train-reverse copa-sl-cer copa-sl ['copa-sl', 'copa-mk', 'copa-ck', 'copa-en', 'copa-hr', 'copa-sr']\n",
      "copa-sr-tor train.trans copa-mk copa-sr ['copa-sl', 'copa-en', 'copa-sl-cer', 'copa-hr', 'copa-ck', 'copa-sr']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 25/25 [00:00<00:00, 74.16ba/s]\n"
     ]
    }
   ],
   "source": [
    "DATADIR='../data'\n",
    "mixmatch_data=make_mixmatch(DATADIR)\n",
    "mixmatch_data.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx_omixmatch_ck'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9427c50a-a98e-4da3-a0e2-da085273b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-ck val.jsonl\n",
      "copa-en val.jsonl\n",
      "copa-hr val.jsonl\n",
      "copa-mk val.jsonl\n",
      "copa-mk val.trans.jsonl\n",
      "copa-sl val.jsonl\n",
      "copa-sl-cer val.jsonl\n",
      "copa-sr val.jsonl\n",
      "copa-sr val.trans.jsonl\n",
      "copa-sr-tor val.jsonl\n",
      "copa-sr-tor val.trans.jsonl\n",
      "copa-hr-ckm test.jsonl\n",
      "copa-sl-cer test.jsonl\n",
      "copa-sr-tor test.jsonl\n"
     ]
    }
   ],
   "source": [
    "DATADIR='../data'\n",
    "val_data=make_all_val_data(DATADIR)\n",
    "val_data.save_to_disk(os.path.join(DATADIR,'all_val'))\n",
    "val_data=load_from_disk(os.path.join(DATADIR,'all_val'))\n",
    "\n",
    "DATADIR='../data/dialect-copa-test'\n",
    "test_data=make_all_test_data(DATADIR)\n",
    "test_data.save_to_disk(os.path.join(DATADIR,'all_test'))\n",
    "test_data=load_from_disk(os.path.join(DATADIR,'all_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee65289-9e06-44aa-a52d-4c40b6ac8850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 42/42 [00:00<00:00, 67.80ba/s]\n"
     ]
    }
   ],
   "source": [
    "DATADIR='../data'\n",
    "dataset1=load_from_disk(os.path.join(DATADIR,'all_train_rev_genx'))\n",
    "dataset2=load_from_disk(os.path.join(DATADIR,'all_train_rev_genx_omixmatch'))\n",
    "dataset=DatasetDict({\"train\": concatenate_datasets([dataset1[\"train\"], dataset2[\"train\"]])})\n",
    "dataset=dataset.shuffle(seed=41)\n",
    "dataset.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx_wmixmatch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8383e8a9-35e6-4358-85c6-c209d5aa585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/datasets/table.py:1315: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Flattening the indices: 100%|██████████| 50/50 [00:00<00:00, 67.00ba/s]\n"
     ]
    }
   ],
   "source": [
    "DATADIR='../data'\n",
    "dataset1=load_from_disk(os.path.join(DATADIR,'all_train_rev_genx_ck'))\n",
    "dataset2=load_from_disk(os.path.join(DATADIR,'all_train_rev_genx_omixmatch_ck'))\n",
    "dataset=DatasetDict({\"train\": concatenate_datasets([dataset1[\"train\"], dataset2[\"train\"]])})\n",
    "dataset=dataset.shuffle(seed=41)\n",
    "dataset.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx_wmixmatch_ck'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbe99669-6732-4469-bbcc-88f91289433c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41352"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a703dd4-1981-4be8-9eff-fa47b698552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_from_disk(os.path.join('../data','all_val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9361d2-2c8d-4fa5-b0dc-f641f4029590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Čovek je otvorio slavinu.',\n",
       " 'choice1': 'WC šolja se napunila vodom.',\n",
       " 'choice2': 'Voda je tekla iz slavine.',\n",
       " 'question': 'effect',\n",
       " 'label': 1,\n",
       " 'idx': 0,\n",
       " 'changed': False}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['copa-sr'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92802266-55d1-4d98-ae42-a98eca0e7acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Атлет је победио у трци.',\n",
       " 'choice1': 'He trained hard and improved his performance.',\n",
       " 'choice2': 'Vsako jutro je šel na sprehod.',\n",
       " 'question': 'cause',\n",
       " 'label': 0,\n",
       " 'idx': 'mix-gpt4-cause-1001'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixmatch_data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9192810-4526-4322-ad0f-d9c811068c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': \"The woman tolerated her friend's difficult behavior.\",\n",
       " 'choice1': 'Ženska je vedela, da ima njena prijateljica težko obdobje.',\n",
       " 'choice2': 'Žena je imala osjećaj da njezin prijatelj iskorištava njezinu dobrotu.',\n",
       " 'question': 'cause',\n",
       " 'label': 0,\n",
       " 'idx': 'mix-1'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2a28f6b-0aad-49d5-8b88-d611c41d1204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"premise\": \"Vojnici se sakrili u džbunja.\", \"choice1\": \"Bili naoružani sas puške.\", \"choice2\": \"Nosili zamaskirane uniforme.\", \"question\": \"cause\", \"label\": 1, \"idx\": 399, \"changed\": false}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5b5ef9a4-b098-4e7c-ae52-baf967ba9ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en train.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train-reverse.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train-reverse.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train-reverse.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train-reverse.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 6/6 [00:00<00:00, 40.61ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_datasets_rev=make_all_train_data_wrev(DATADIR)\n",
    "DATADIR='../data'\n",
    "train_datasets_rev.save_to_disk(os.path.join(DATADIR,'all_train_rev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bafb6b2d-5dac-4f23-b2d3-b6a4d18cacf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en train-genx.jsonl\n",
      "copa-en train.jsonl\n",
      "copa-hr train-genx.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train-genx.jsonl\n",
      "copa-sl train-reverse.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train-reverse.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train-genx.jsonl\n",
      "copa-sr train-reverse.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train-reverse.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 21/21 [00:00<00:00, 61.87ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_datasets_rev=make_all_train_data_wrev_wgenx(DATADIR)\n",
    "DATADIR='../data'\n",
    "train_datasets_rev.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e067d412-1d73-4255-8ecd-3236721954cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-ck train-genx.jsonl\n",
      "copa-ck train.jsonl\n",
      "copa-en train-genx.jsonl\n",
      "copa-en train.jsonl\n",
      "copa-hr train-genx.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train-genx.jsonl\n",
      "copa-sl train-reverse.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train-reverse.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train-genx.jsonl\n",
      "copa-sr train-reverse.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train-reverse.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 25/25 [00:00<00:00, 55.00ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_datasets_rev=make_all_train_data_wrev_wgenx(DATADIR)\n",
    "DATADIR='../data'\n",
    "train_datasets_rev.save_to_disk(os.path.join(DATADIR,'all_train_rev_genx_ck'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdcfd4e6-3ac6-40d5-aca3-46b7a9c5dc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en train.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-mk train.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n",
      "synthetic\n",
      "synthetic copa-en-train-general.jsonl\n",
      "synthetic copa-en-train-similar.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 5/5 [00:00<00:00, 22.61ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_datasets_syn=make_all_train_data_synthetic(DATADIR)\n",
    "DATADIR='../data'\n",
    "train_datasets_syn.save_to_disk(os.path.join(DATADIR,'all_train_syn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59b78f9d-c14a-48b0-ac35-0fb89ea734a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
       "        num_rows: 4679\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05079027-52c8-41dd-81bc-225497fd1c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx'],\n",
       "    num_rows: 400\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_datasets['copa-en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa30f67f-f165-498b-8ada-1a43a309bb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-hr-ckm train.jsonl\n",
      "['.ipynb_checkpoints', 'train.jsonl']\n"
     ]
    }
   ],
   "source": [
    "def get_reverse(x):\n",
    "    label_dict={0:'choice1',1:'choice2'}\n",
    "    premise=x[label_dict[x['label']]]\n",
    "    x[label_dict[x['label']]]=x['premise']\n",
    "    x['premise']=premise\n",
    "    if x['question']=='cause':\n",
    "        x['question']='effect'\n",
    "    else:\n",
    "        x['question']='cause'\n",
    "    return x\n",
    "\n",
    "import json\n",
    "DATADIR=\"../data\"\n",
    "for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa') and str(f).endswith(\"hr-ckm\"):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train.jsonl'):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    print(os.listdir(os.path.join(DATADIR,f)))\n",
    "                    reverse_line_dicts=[get_reverse(line.copy()) for line in line_dicts]\n",
    "                    dest_file=os.path.join(DATADIR,f,'train-reverse.jsonl' )\n",
    "                    output_file = open(dest_file, 'w',encoding='utf-8')\n",
    "                    for dic in reverse_line_dicts:\n",
    "                        json.dump(dic, output_file,ensure_ascii=False) \n",
    "                        output_file.write(\"\\n\")\n",
    "                    output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "acf20d76-8189-4c3a-b9f8-f5e25b9c2a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_line_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "457b952c-acb0-49cf-9017-bc6780cf7395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(line_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d2b0a24-cebb-4296-bc98-7e8d43b40b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'The sun was rising.',\n",
       " 'choice1': 'My body cast a shadow over the grass.',\n",
       " 'choice2': 'The grass was cut.',\n",
       " 'question': 'effect',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6eafb1e-5c5a-43af-b86a-f35e5e757466",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR='../data'\n",
    "dataset=load_from_disk(os.path.join(DATADIR,'all_train_rev'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "791f7113-1058-4446-87c5-bb6e1efd2abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
       "        num_rows: 4400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f44ec42e-adbf-4f11-aebe-c072d7800d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['validation']=val_datasets['copa-en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "656e2f15-bbfe-4e8e-ac14-2afab0a6fe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-hr Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-mk Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sl Dataset({\n",
      "    features: ['choice1', 'choice2', 'idx', 'label', 'premise', 'question'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sl-cer Dataset({\n",
      "    features: ['choice1', 'choice2', 'idx', 'label', 'premise', 'question'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sr Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n",
      "copa-sr-tor Dataset({\n",
      "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'],\n",
      "    num_rows: 400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for r in val_datasets:\n",
    "    print(r,val_datasets[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96f306ad-62ec-403f-82a7-79cb66a7e072",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR=\"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56887e71-bd0d-453a-8bd6-275e022ded46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 400\n",
      "val size: 100\n"
     ]
    }
   ],
   "source": [
    "dataset=load_datasets(\"copa-hr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "687f4176-254f-4602-a88e-763963f5c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# lines = []\n",
    "\n",
    "# with open(os.path.join(DATADIR,'copa-hr','train.jsonl')) as f:\n",
    "#     lines = f.read().splitlines()\n",
    "\n",
    "# line_dicts = [json.loads(line) for line in lines]\n",
    "# df_final = pd.DataFrame(line_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1188f51-d21c-43ab-8c90-61e65100f81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'changed'], dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e53aac47-3214-4a3a-9d14-693cfff68f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"Croatian\"\n",
    "preamble = f\"\"\"You are a helpful following assistant whose goal is to select the preferred (least wrong) output for a given instruction in {lang}.\"\"\"\n",
    "\n",
    "prompt_template=\"\"\"Instruction: Given the premise, \"\"{premise}\"\", What is the correct {question}?\n",
    "{question} A: {choice1}\n",
    "{question} B: {choice2}\n",
    "Correct {question}: {correct_answer}\"\"\"\n",
    "\n",
    "choices=[\"A\",\"B\"]\n",
    "\n",
    "def get_few_shot_examples(dataset, fs_per_label=1, seed=42):\n",
    "    labels = list(set(dataset[\"label\"]))\n",
    "    few_shot_examples = []\n",
    "    for label in labels:\n",
    "        label_examples = dataset.filter(lambda example: example[\"label\"] == label and example[\"question\"]=='cause')\n",
    "        # shuffle the examples\n",
    "        label_examples = label_examples.shuffle(seed=seed)\n",
    "        # get the first fs_per_label examples\n",
    "        label_examples = label_examples.select(\n",
    "            range(min(fs_per_label, len(label_examples)))\n",
    "        )\n",
    "        few_shot_examples += [example for example in label_examples]\n",
    "        \n",
    "        label_examples = dataset.filter(lambda example: example[\"label\"] == label and example[\"question\"]=='effect')\n",
    "        # shuffle the examples\n",
    "        label_examples = label_examples.shuffle(seed=seed)\n",
    "        # get the first fs_per_label examples\n",
    "        label_examples = label_examples.select(\n",
    "            range(min(fs_per_label, len(label_examples)))\n",
    "        )\n",
    "        few_shot_examples += [example for example in label_examples]\n",
    "\n",
    "    # Shuffle the few shot examples\n",
    "    random.shuffle(few_shot_examples)\n",
    "    return few_shot_examples\n",
    "\n",
    "def construct_prompt(ds_examples):\n",
    "    def example_to_prompt(example, add_label=True):\n",
    "        ex_prompt = f\"Sentence: {example['text']}\\n\"\n",
    "        if add_label:\n",
    "            ex_prompt += f\"Label: {example['label']}\\n\"\n",
    "        return ex_prompt\n",
    "\n",
    "    # To Do: Add domain of the text in the instruction like \"In this task you given text from {domain}\n",
    "\n",
    "    # Format the first five rows as examples for 5-shot prompting\n",
    "    prompt_examples = \"\\n\\n\".join([ prompt_template.format(**d,correct_answer=choices[int(d[\"label\"])-1]) for d in ds_examples])\n",
    "    prompt_examples=preamble+\"\\n\\n\\n\"+prompt_examples\n",
    "    return prompt_examples\n",
    "#     for example in fs_examples:\n",
    "#         prompt += example_to_prompt(example, add_label=True)\n",
    "#         prompt += \"\\n\"\n",
    "\n",
    "#     if not prompt_for_each_label:\n",
    "#         prompt += example_to_prompt(test_example, add_label=False)\n",
    "#         return prompt\n",
    "#     else:\n",
    "#         prompts = [\n",
    "#             prompt + example_to_prompt(test_example, add_label=True) for label in labels\n",
    "#         ]\n",
    "#         gold_label_idx = labels.index(test_example[\"label\"])\n",
    "#         return prompts, gold_label_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2529b2d6-5138-4554-9dfc-67eb3b10abaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 62826.60 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 88506.10 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 88124.89 examples/s]\n",
      "Filter: 100%|██████████| 400/400 [00:00<00:00, 85702.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fs_examp=get_few_shot_examples(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8cab222-be9a-4704-b107-9cd1909bfba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_prompt=construct_prompt(fs_examp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fab0d424-6440-4ca0-b1f0-6fdb2a00657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful following assistant whose goal is to select the preferred (least wrong) output for a given instruction in Croatian.\n",
      "\n",
      "\n",
      "Instruction: Given the premise, \"\"Ometen sam u razgovoru sa ženom.\"\", What is the correct cause?\n",
      "cause A: Svi su u sobi govorili.\n",
      "cause B: Žena je pričala smiješnu priču.\n",
      "Correct cause: B\n",
      "\n",
      "Instruction: Given the premise, \"\"Mačka je prela.\"\", What is the correct cause?\n",
      "cause A: Ogrebla me.\n",
      "cause B: Pomazio sam je.\n",
      "Correct cause: A\n",
      "\n",
      "Instruction: Given the premise, \"\"Ravnatelj škole uveo je pravila oblačenja.\"\", What is the correct effect?\n",
      "effect A: Učenici su se pobunili protiv te odluke.\n",
      "effect B: Učenici su izbačeni iz škole.\n",
      "Correct effect: B\n",
      "\n",
      "Instruction: Given the premise, \"\"Starija žena doživjela je moždani udar.\"\", What is the correct effect?\n",
      "effect A: Ženina kći došla joj je očistiti kuću.\n",
      "effect B: Ženina kći uselila se da bi vodila brigu o njoj.\n",
      "Correct effect: A\n"
     ]
    }
   ],
   "source": [
    "print(fs_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53eb3b6-6da5-4ef3-bb86-3fc44459aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.01s/it]\n",
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n",
    "                                                  token=\"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\", \n",
    "                                                 cache_dir='../models/Llama-2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9d5e00-9bfc-48fe-b6f0-4c742ddea37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../models/mala-500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9cca3c4-8935-4a40-8f97-9fabd5a5a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.resize_token_embeddings(260164)\n",
    "model = PeftModel.from_pretrained(base_model, '../models/mala-500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5332f90-c091-4c96-a6ce-f995277fe679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(260164, 4096)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(260164, 4096)\n",
       "          )\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=4096, out_features=260164, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=4096, out_features=260164, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd3757d-d676-479d-b024-1d49513b290b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "816d807e-8ce4-4c8e-9441-06d15f91130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:04<00:00, 21.50s/it]\n",
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-model', '--model', type=str, help='data folder location', required=True)\n",
    "    args = parser.parse_args()\n",
    "    return args  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # args = get_arguments()\n",
    "    # print(args)\n",
    "    # model_name=args.model\n",
    "    model_name=\"llama2_7b_chat\"\n",
    "    \n",
    "    model_configs={\n",
    "    'llama2_7b_chat':{\n",
    "        'model_path':'/projects/antonis/models/LLaMA-v2/Llama2-chat-hf/7B'\n",
    "    },\n",
    "    'llama2_7b':{\n",
    "        'model_path':'../models/Llama-2-7b'\n",
    "    },\n",
    "    }\n",
    "\n",
    "    load_4bit=False\n",
    "    load_8bit=False\n",
    "\n",
    "\n",
    "    model_path=model_configs[model_name]['model_path']\n",
    "    model, tokenizer = get_model(model_path,load_4bit=False,load_8bit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fd9d8d-7129-4ded-a0af-d7f68a4b7a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:55<00:00,  5.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"../models/aya-101\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3e76a8c-7ce5-4b3e-983a-37689c6ace00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a0d138-43b9-481e-9e48-4769abc4683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'B', 'A', 'B', 'cause B', 'A', 'cause A', 'cause A', 'cause B', 'A']\n"
     ]
    }
   ],
   "source": [
    "encodings=tokenizer(all_val_prompts[0:10], return_tensors=\"pt\", padding='longest', truncation=False).to(\"cuda\")\n",
    "output_ids=model.generate(**encodings, **gen_config)\n",
    "responses=tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f03726-21fc-4a25-b7e1-d8ab20b53cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.1,\n",
    "                \"repetition_penalty\": 1.18,\n",
    "                \"top_k\": 40,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\": 5,\n",
    "                \"pad_token_id\": tokenizer.eos_token_id\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92d8143e-4bc2-43b0-8e33-82146cc6ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result(prompts,gen_config,model_name='aya',bs=4):\n",
    "    all_response=[]\n",
    "    all_response_raw=[]\n",
    "    end=len(prompts)\n",
    "    for start in tqdm(range(0,end,bs)):\n",
    "        stop=min(start+bs,len(prompts)-1)\n",
    "        if start<stop:\n",
    "            prompts_batch=prompts[start:stop]\n",
    "            encodings=tokenizer(prompts_batch, return_tensors=\"pt\", padding='longest', truncation=False).to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(**encodings, **gen_config)\n",
    "            responses=tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "            for i,response_raw in enumerate(responses):\n",
    "                sample_no=i+start\n",
    "                if model_name!='aya':\n",
    "                    response=response_raw[len(prompts[sample_no]):]\n",
    "                    response=response.split(\"\\n\")[0].strip() if \"\\n\" in response else response.strip()\n",
    "                else:\n",
    "                    response=response_raw[-1]\n",
    "                all_response.append(response)\n",
    "                all_response_raw.append(response_raw)\n",
    "                \n",
    "    return all_response_raw,all_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82686399-36be-4131-adb8-8672e48d8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_prompts=[]\n",
    "all_val_labels=[]\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "for row in dataset['val']:\n",
    "    prompt=(fs_prompt + \"\\n\\n\" + prompt_template.format(**row, correct_answer=\"\")).strip()\n",
    "    all_val_prompts.append(prompt)\n",
    "    all_val_labels.append(row['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd79401e-7afb-4d80-94fb-2edc80783c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:28<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "all_response_raw,all_response=generate_result(all_val_prompts,gen_config,'aya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fd8991f-6833-4c2a-bbce-fc2aa78f0669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7474747474747475\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i,res in enumerate(all_response):\n",
    "    if res in choices:\n",
    "        if choices.index(res)==all_val_labels[i]:\n",
    "            count+=1\n",
    "acc=count/len(all_response)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f450a8-f471-4020-8f4c-45b3e184b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ffaisal/dialect-copa/vnv/vnv_copa/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13355c4e-0c47-410d-b65a-a2156fc09315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union\n",
    "import time\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.tokenization_utils_base import PaddingStrategy, PreTrainedTokenizerBase\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bf6ec0fe-4f68-49c3-84bb-2b6a6479731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 400\n",
      "val size: 100\n"
     ]
    }
   ],
   "source": [
    "datasets=load_datasets(\"copa-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5c3059c7-474c-48e1-a4fa-e1b0125dd963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'My body cast a shadow over the grass.',\n",
       " 'choice1': 'The sun was rising.',\n",
       " 'choice2': 'The grass was cut.',\n",
       " 'question': 'cause',\n",
       " 'label': 0,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5834547a-ba91-4a94-91f9-e3d2324f064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"classla/bcms-bertic\"\n",
    "config = AutoConfig.from_pretrained(model_name\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name\n",
    "    )\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1c141feb-6435-4a08-8971-57825d53b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_name = \"premise\"\n",
    "question_header_name = \"question\"\n",
    "ending_names = [f\"choice{i}\" for i in [1, 2]]\n",
    "num_answers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "73aeaece-8f54-4b5b-84f6-cad1df28b4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Map: 100%|██████████| 400/400 [00:00<00:00, 7589.58 examples/s]\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 6192.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [\n",
    "      [f\"{context} What was the {question}?\"] * num_answers for context, question in zip(examples[context_name], examples[question_header_name])\n",
    "    ]\n",
    "    second_sentences = [\n",
    "            [f\"{examples[end][i]}\" for end in ending_names] for i, _ in enumerate(first_sentences)\n",
    "        ]\n",
    "    # Flatten out\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Un-flatten\n",
    "    # unflattened = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "    unflattened = {k: [v[i : i + num_answers] for i in range(0, len(v), num_answers)] for k, v in tokenized_examples.items()}\n",
    "    return unflattened\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "preprocess_function,\n",
    "batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fee736a2-9aa4-4ec3-b9c1-87ba8da878d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = (\n",
    "    DataCollatorForMultipleChoice(tokenizer)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "58f692ad-2e0f-47d1-9bf5-373c79400c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_predictions):\n",
    "        predictions, label_ids = eval_predictions\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "55881a6d-f622-4ee6-9c85-d2b5f4e31586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "136d49b8-77d7-4c7a-903d-65900aaa8cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'choice1', 'choice2', 'question', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "66f8db4c-ed11-4057-817e-62c044004920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13.1622, 'train_samples_per_second': 91.17, 'train_steps_per_second': 11.396, 'train_loss': 0.6914518229166666, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7027a5e2-ba86-488b-9829-910868a38e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6911947727203369, 'eval_accuracy': 0.47999998927116394, 'eval_runtime': 0.3688, 'eval_samples_per_second': 271.154, 'eval_steps_per_second': 35.25, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "esults = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3a03fae0-c29a-4b50-a4ed-86ab9067b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng---\n",
    "# mbert=55\n",
    "# bert=66\n",
    "# xlmr=54\n",
    "# bertic=48\n",
    "# llama2-chat=75\n",
    "# mala-500=50\n",
    "# aya=80\n",
    "\n",
    "#copa-hr\n",
    "# bert=55\n",
    "# mbert=57\n",
    "# xlm-r=54\n",
    "# bertic=64\n",
    "# llama2 = 50\n",
    "# aya= 75\n",
    "\n",
    "##-modeling from scratch/finetuning with llm-augmented data generation?\n",
    "# improve dialectal performance with the aid of llm\n",
    "# trend\n",
    "# - previously: llm works or not, benchmarking\n",
    "# - current: reinventing mbert experiments: adding augmented data to improve the performance, llm generated data to train\n",
    "# - next: Think outside of the traditional benchmarking?\n",
    "# -     taking high resource and it's low-resource counterparts. \n",
    "        #Use prompt techniques \n",
    "        #See results and use the insight from there to use lora training. \n",
    "# - low-resource data is the bottolneck here. how much augmentation/synthetic data actually help. In which cases, it must fail?\n",
    "# - what extra thing can llm does which mbert cant do except synthetic data generation\n",
    "\n",
    "\n",
    "1. ft on all other data at once/one by one (mbert, xlm-r, bertic)\n",
    "2. get result from aya on all of them (2 cause, 4 cause in prompt)\n",
    "3. for a give test example, try to choose most similar training examples, in prompt provide info about dialect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f42f01-beeb-4ea6-99fc-c2db80fa5df4",
   "metadata": {},
   "source": [
    "- first I tried reformating the promt to increase eng val score on aya (from 80 to 86)\n",
    "- tried using sentence trasnformar to prompt with similar sentences 85\n",
    "- triend all data finetuning bertic model-> best result\n",
    "- generate synthetic data for english using 4 example in few shot prompting\n",
    "- generate synthetic data for english using 4 similar example in few shot prompting\n",
    "- adding these 650 extra train example with original train: worse result (50%)\n",
    "- reversing the sl-cer reverse example- improve result for sr 1%, decrease a bit for all other\n",
    "- reverse the sl-cer and sr: worse result for all..around 50%\n",
    "- then ran again with reversing the sl-cer reverse example-best result\n",
    "- changed the gradient acc step from 4 to 1: best result\n",
    "- also, found 1 chakavian dialect dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ff9f33-1567-4f16-970c-88b24f981cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATADIR='../data/genx_xcopa'\n",
    "with open(os.path.join(DATADIR,f\"eng.jsonl\"), encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    line_dicts = [json.loads(line) for line in lines]\n",
    "    all_lines=[]\n",
    "    for line in line_dicts:\n",
    "        all_lines.append(line['premise'])\n",
    "        all_lines.append(line['choice1'])\n",
    "        all_lines.append(line['choice2'])\n",
    "with open(os.path.join(DATADIR,f\"eng.txt\"), 'w') as f:\n",
    "    for line in all_lines:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262fe019-d7b6-4608-a1e1-9adb1147ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATADIR='../data/genx_xcopa'\n",
    "with open(os.path.join(DATADIR,f\"eng.jsonl\"), encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "    line_dicts = [json.loads(line) for line in lines]\n",
    "\n",
    "dest_lang='mkd'    \n",
    "src_file=\"../data/genx_xcopa/eng.txt\"\n",
    "dest_file=f\"../data/genx_xcopa/{dest_lang}.txt\"\n",
    "with open(src_file) as f:\n",
    "    source_sents = f.read().splitlines()\n",
    "with open(dest_file) as f:\n",
    "    dest_sents = f.read().splitlines()\n",
    "    \n",
    "trans_dict={}\n",
    "for i,s in enumerate(source_sents):\n",
    "    trans_dict[s]=dest_sents[i]\n",
    "new_line_dicts=[]\n",
    "for l in line_dicts:\n",
    "    new_line_dicts.append({\n",
    "        'premise':trans_dict[l['premise']],\n",
    "        'choice1':trans_dict[l['choice1']],\n",
    "        'choice2':trans_dict[l['choice2']],\n",
    "        'question':l['question'],\n",
    "        'idx':l['idx'],\n",
    "        'label':l['label']\n",
    "    })\n",
    "\n",
    "import json\n",
    "dest_file=f\"../data/genx_xcopa/{dest_lang}.jsonl\"\n",
    "output_file = open(dest_file,'w',encoding='utf-8')\n",
    "for dic in new_line_dicts:\n",
    "    json.dump(dic, output_file, ensure_ascii=False) \n",
    "    output_file.write(\"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30891b35-efc4-4086-ab01-c408afd1b765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Вулканот еруптира.',\n",
       " 'choice1': 'Височината на планината се удвои.',\n",
       " 'choice2': 'Лавата течеше низ планината.',\n",
       " 'question': 'effect',\n",
       " 'idx': 'gpt4-effect-207',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_line_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b0282c-f684-462a-baa2-d2409f981ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9173"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76365262-54a2-401b-9e51-37b957a86e92",
   "metadata": {},
   "source": [
    "#### sr-tor and sl-cer translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88334d66-8008-4f77-b49f-c14be01f3e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "import os\n",
    "\n",
    "all_lines={}\n",
    "langs=['copa-sr','copa-sr-tor','copa-sl-cer','copa-sl','copa-hr','copa-mk']\n",
    "for lang in langs:\n",
    "    DATADIR=f'../data/{lang}'\n",
    "    all_lines[lang]=[]\n",
    "    if 'copa-sr' in lang or 'copa-mk' in lang:\n",
    "        split='train.trans'\n",
    "    else:\n",
    "        split='train'\n",
    "    with open(os.path.join(DATADIR,f\"train.jsonl\"), encoding=\"utf-8\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "        for line in line_dicts:\n",
    "            all_lines[lang].append(line['premise'])\n",
    "            all_lines[lang].append(line['choice1'])\n",
    "            all_lines[lang].append(line['choice2'])\n",
    "    if 'copa-sr' in lang or 'copa-mk' in lang:\n",
    "        split='val.trans'\n",
    "    else:\n",
    "        split='val'\n",
    "    with open(os.path.join(DATADIR,f\"val.jsonl\"), encoding=\"utf-8\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "        for line in line_dicts:\n",
    "            all_lines[lang].append(line['premise'])\n",
    "            all_lines[lang].append(line['choice1'])\n",
    "            all_lines[lang].append(line['choice2'])\n",
    "    with open(os.path.join('../data/all_sentences',f\"{lang}_orig.txt\"), 'w') as f:\n",
    "        for line in all_lines[lang]:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "all_lines_genx={}\n",
    "langs=['copa-sr','copa-sl','copa-hr']\n",
    "for lang in langs:\n",
    "    DATADIR=f'../data/{lang}'\n",
    "    all_lines_genx[lang]=[]\n",
    "    with open(os.path.join(DATADIR,f\"train-genx.jsonl\"), encoding=\"utf-8\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "        for line in line_dicts:\n",
    "            all_lines_genx[lang].append(line['premise'])\n",
    "            all_lines_genx[lang].append(line['choice1'])\n",
    "            all_lines_genx[lang].append(line['choice2'])\n",
    "    with open(os.path.join('../data/all_sentences',f\"{lang}_genx.txt\"), 'w') as f:\n",
    "        for line in all_lines_genx[lang]:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5dc9d2-4824-4727-8be8-50c1c92e0cd2",
   "metadata": {},
   "source": [
    "#### create chakavian train from hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fd40d77-0300-4294-a929-eaacf3d2672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr2ckm={}\n",
    "hr=[]\n",
    "lang='copa-hr'\n",
    "with open(os.path.join('../data/all_sentences',f\"{lang}_orig.txt\"),'r') as fh:\n",
    "    for line in fh:\n",
    "        hr.append(line.split(\"\\n\")[0])\n",
    "\n",
    "ckm=[]\n",
    "lang='copa-hr-ckm'\n",
    "with open(os.path.join('../data/all_sentences',f\"{lang}_orig.txt\"),'r') as fh:\n",
    "    for line in fh:\n",
    "        ckm.append(line.split(\"\\n\")[0])\n",
    "\n",
    "for h,c in zip(hr,ckm):\n",
    "    hr2ckm[h]=c\n",
    "\n",
    "lang='copa-hr'\n",
    "DATADIR=f'../data/{lang}'    \n",
    "with open(os.path.join(DATADIR,f\"train.jsonl\"), encoding=\"utf-8\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "        for i,line in enumerate(line_dicts):\n",
    "            line_dicts[i]['premise']=hr2ckm[line_dicts[i]['premise']]\n",
    "            line_dicts[i]['choice1']=hr2ckm[line_dicts[i]['choice1']]\n",
    "            line_dicts[i]['choice2']=hr2ckm[line_dicts[i]['choice2']]\n",
    "import json\n",
    "\n",
    "dest_file=f\"../data/copa-hr-ckm/train.jsonl\"\n",
    "output_file = open(dest_file,'w',encoding='utf-8')\n",
    "for dic in line_dicts:\n",
    "    json.dump(dic, output_file, ensure_ascii=False) \n",
    "    output_file.write(\"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d7bdfe8-2557-425e-b78a-b763fb38ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moje je tijelo bacalo sjenu na travu.',\n",
       " 'Sunce je izlazilo.',\n",
       " 'Trava je bila pokošena.',\n",
       " 'Žena je tolerirala problematično ponašanje svojeg prijatelja.',\n",
       " 'Žena je znala da njezin prijatelj prolazi kroz teško razdoblje.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c0e18-b9c2-4a95-9640-55c349b39ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f28e6fa-dd2f-4fd4-b371-295f971a81d3",
   "metadata": {},
   "source": [
    "### transliterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6952576e-a591-4b13-9587-5d462b692ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyrtranslit\n",
    "# cyrtranslit.to_latin(\"Моето летачко возило е полно со јагули\", \"mk\")\n",
    "\n",
    "all_lines_genx={}\n",
    "langs=['copa-sr','copa-mk']\n",
    "for lang in langs:\n",
    "    DATADIR=f'../data/{lang}'\n",
    "    all_lines_genx[lang]=[]\n",
    "    with open(os.path.join(DATADIR,f\"train-genx.jsonl\"), encoding=\"utf-8\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "    for i,line in enumerate(line_dicts):\n",
    "        line_dicts[i]['premise']=cyrtranslit.to_latin(line_dicts[i]['premise'], \"sr\")\n",
    "        line_dicts[i]['choice1']=cyrtranslit.to_latin(line_dicts[i]['choice1'], \"sr\")\n",
    "        line_dicts[i]['choice2']=cyrtranslit.to_latin(line_dicts[i]['choice2'], \"sr\")\n",
    "import json\n",
    "dest_file=f\"{DATADIR}/train-genx.trans.jsonl\"\n",
    "output_file = open(dest_file,'w',encoding='utf-8')\n",
    "for dic in line_dicts:\n",
    "    json.dump(dic, output_file, ensure_ascii=False) \n",
    "    output_file.write(\"\\n\")\n",
    "output_file.close()\n",
    "\n",
    "\n",
    "langs=['copa-sr','copa-sr-tor','copa-mk']\n",
    "for lang in langs:\n",
    "    DATADIR=f'../data/{lang}'\n",
    "    all_lines_genx[lang]=[]\n",
    "    with open(os.path.join(DATADIR,f\"train-reverse.jsonl\"), encoding=\"utf-8\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        line_dicts = [json.loads(line) for line in lines]\n",
    "    for i,line in enumerate(line_dicts):\n",
    "        line_dicts[i]['premise']=cyrtranslit.to_latin(line_dicts[i]['premise'], \"sr\")\n",
    "        line_dicts[i]['choice1']=cyrtranslit.to_latin(line_dicts[i]['choice1'], \"sr\")\n",
    "        line_dicts[i]['choice2']=cyrtranslit.to_latin(line_dicts[i]['choice2'], \"sr\")\n",
    "import json\n",
    "dest_file=f\"{DATADIR}/train-reverse.trans.jsonl\"\n",
    "output_file = open(dest_file,'w',encoding='utf-8')\n",
    "for dic in line_dicts:\n",
    "    json.dump(dic, output_file, ensure_ascii=False) \n",
    "    output_file.write(\"\\n\")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7332df46-26e0-406d-be2e-d2fd52ace365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Vulkan je eruptovao.',\n",
       " 'choice1': 'Visina planine se udvostručila.',\n",
       " 'choice2': 'Lava je tekla s planine.',\n",
       " 'question': 'effect',\n",
       " 'idx': 'gpt4-effect-207',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a41f25-68bf-43eb-8ad0-c70343e31e8e",
   "metadata": {},
   "source": [
    "#### FINAL TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8658cd-8c20-4000-8879-ee98d55921f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa-en train-genx.jsonl\n",
      "copa-en train.jsonl\n",
      "copa-hr train-genx.jsonl\n",
      "copa-hr train-reverse.jsonl\n",
      "copa-hr train.jsonl\n",
      "copa-hr-ckm train-reverse.jsonl\n",
      "copa-hr-ckm train.jsonl\n",
      "copa-mk train-genx.cyrl.jsonl\n",
      "copa-mk train-genx.trans.jsonl\n",
      "copa-mk train-reverse.cyrl.jsonl\n",
      "copa-mk train-reverse.trans.jsonl\n",
      "copa-mk train.cyrl.jsonl\n",
      "copa-mk train.trans.jsonl\n",
      "copa-sl train-genx.jsonl\n",
      "copa-sl train-reverse.jsonl\n",
      "copa-sl train.jsonl\n",
      "copa-sl-cer train-reverse.jsonl\n",
      "copa-sl-cer train.jsonl\n",
      "copa-sr train-genx.cyrl.jsonl\n",
      "copa-sr train-genx.trans.jsonl\n",
      "copa-sr train-reverse.cyrl.jsonl\n",
      "copa-sr train-reverse.trans.jsonl\n",
      "copa-sr train.cyrl.jsonl\n",
      "copa-sr train.trans.jsonl\n",
      "copa-sr-tor train-reverse.cyrl.jsonl\n",
      "copa-sr-tor train-reverse.trans.jsonl\n",
      "copa-sr-tor train.cyrl.jsonl\n",
      "copa-sr-tor train.trans.jsonl\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datasets, random\n",
    "\n",
    "DATADIR='../data'\n",
    "# def get_all_train_data(DATADIR):\n",
    "#     datasets={'train':[]}\n",
    "#     for f in os.listdir(DATADIR):\n",
    "#         if str(f).startswith('copa'):\n",
    "#             for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "#                 if str(f1).startswith('train') and 'reverse' not in str(f1) and 'genx' not in str(f1):\n",
    "#                     print(f,f1)\n",
    "#                     with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "#                         lines = f2.read().splitlines()\n",
    "#                     line_dicts = [json.loads(line) for line in lines]\n",
    "#                     datasets['train'].extend(line_dicts)\n",
    "#     datasets['train'] = Dataset.from_pandas(pd.DataFrame(data=datasets['train']))\n",
    "#     datasets = DatasetDict(datasets)\n",
    "#     datasets=datasets.shuffle(seed=41)\n",
    "#     return datasets\n",
    "\n",
    "def get_all_train_data(DATADIR):\n",
    "    dataset={}\n",
    "    for f in os.listdir(DATADIR):\n",
    "        if str(f).startswith('copa'):\n",
    "            for f1 in os.listdir(os.path.join(DATADIR,f)):\n",
    "                if str(f1).startswith('train'):\n",
    "                    print(f,f1)\n",
    "                    with open(os.path.join(DATADIR,f,f1), encoding=\"utf-8\") as f2:\n",
    "                        lines = f2.read().splitlines()\n",
    "                    line_dicts = [json.loads(line) for line in lines]\n",
    "                    for i,line in enumerate(line_dicts):\n",
    "                        line_dicts[i]['idx']=str(line_dicts[i]['idx'])\n",
    "                    name=f'{str(f)}:{str(f1)}'\n",
    "                    dataset[name]=line_dicts\n",
    "    return dataset\n",
    "dataset=get_all_train_data(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c6b562-9d89-470e-bc9a-8d796cd84132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def combine(dataset,lang_list,type):\n",
    "    def mixmatch(langs):\n",
    "        for i,dirs in enumerate(langs):\n",
    "            # if i>0:\n",
    "            #     break\n",
    "            odirs=langs[:i]+langs[i+1:]\n",
    "            for j,data in enumerate(dataset[dirs]):\n",
    "                # print(i,j)\n",
    "                # if j>5:\n",
    "                #     break\n",
    "                c1choice=random.choice(odirs)\n",
    "                odirs1=odirs.copy()\n",
    "                odirs1.remove(c1choice)\n",
    "                c2choice=random.choice(odirs1)\n",
    "                # print(dirs,c1choice,c2choice)\n",
    "                ndata=data.copy()\n",
    "                ndata['premise']=dataset[dirs][j]['premise']\n",
    "                ndata['choice1']=dataset[c1choice][j]['choice1']\n",
    "                ndata['choice2']=dataset[c2choice][j]['choice2']\n",
    "                combine_data.append(ndata)\n",
    "    combine_data=[]\n",
    "    if type=='orgl' or type=='orglc':\n",
    "        for i,data in enumerate(lang_list):\n",
    "            combine_data.extend(dataset[lang_list[i]])\n",
    "    if type=='orglc_omix' or type=='orgl_omix':\n",
    "        train_splits=['train','train-genx','train-reverse']\n",
    "        all_dirs=lang_list\n",
    "        tr_only=[dirs for dirs in all_dirs if (not dirs.split(':')[1].startswith('train-genx') and not dirs.split(':')[1].startswith('train-reverse'))]\n",
    "        gen_only=[dirs for dirs in all_dirs if dirs.split(':')[1].startswith('train-genx')]\n",
    "        rev_only=[dirs for dirs in all_dirs if dirs.split(':')[1].startswith('train-reverse')]\n",
    "        shuffle(tr_only)\n",
    "        mixmatch(tr_only)\n",
    "        shuffle(gen_only)\n",
    "        mixmatch(gen_only)\n",
    "        shuffle(rev_only)\n",
    "        mixmatch(rev_only)\n",
    "    # if type=='orgl_omix':\n",
    "    #     train_splits=['train','train-genx','train-reverse']\n",
    "    #     all_dirs=lang_list\n",
    "    #     tr_only=[dirs for dirs in all_dirs if (not dirs.split(':')[1].startswith('train-genx') and not dirs.split(':')[1].startswith('train-reverse'))]\n",
    "    #     gen_only=[dirs for dirs in all_dirs if dirs.split(':')[1].startswith('train-genx')]\n",
    "    #     rev_only=[dirs for dirs in all_dirs if dirs.split(':')[1].startswith('train-reverse')]\n",
    "    #     shuffle(tr_only)\n",
    "    #     mixmatch(tr_only)\n",
    "    #     shuffle(gen_only)\n",
    "    #     mixmatch(gen_only)\n",
    "    #     shuffle(rev_only)\n",
    "    #     mixmatch(rev_only)\n",
    "    \n",
    "    print(len(combine_data))\n",
    "    return combine_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c9efe9-11ab-43b8-92a0-bc751c50cfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34783\n",
      "24845\n",
      "34783\n",
      "24845\n"
     ]
    }
   ],
   "source": [
    "all_lang=list(dataset.keys())\n",
    "all_latn=[x for x in dataset.keys() if  'cyrl' not in x]\n",
    "all_cyrl=[x for x in dataset.keys() if  'cyrl' in x]\n",
    "all_rev=[x for x in dataset.keys() if  'reverse' in x]\n",
    "all_genx=[x for x in dataset.keys() if  'genx' in x]\n",
    "\n",
    "orglc=combine(dataset.copy(),all_lang,'orglc')\n",
    "orgl=combine(dataset.copy(),all_latn,'orgl')\n",
    "orglc_omix=combine(dataset.copy(),all_lang,'orglc_omix')\n",
    "orgl_omix=combine(dataset.copy(),all_latn,'orgl_omix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147dbb49-a0ea-46c3-a99d-e8d62a08548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 8\n"
     ]
    }
   ],
   "source": [
    "sr_tor_all=[x for x in dataset.keys() if  'sr' in x]\n",
    "sr_tor_latn=[x for x in all_latn if  'sr' in x]\n",
    "sl_cer_latn=[x for x in all_latn if  'sl' in x]\n",
    "hr_ckm_latn=[x for x in all_latn if  'hr' in x]\n",
    "mk_hr_ckm_all=[x for x in dataset.keys() if  'hr' in x or 'mk' in x]\n",
    "mk_hr_ckm_latn=[x for x in all_latn if  'hr' in x or 'mk' in x]\n",
    "print(len(mk_hr_ckm_all),len(mk_hr_ckm_latn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4be272-0214-40c5-bee7-bbf9f309da14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32214\n",
      "43521\n",
      "32214\n",
      "43521\n",
      "32214\n",
      "39752\n",
      "32214\n",
      "32214\n",
      "32214\n",
      "39752\n",
      "32214\n",
      "32214\n"
     ]
    }
   ],
   "source": [
    "orglc_sr_tor=combine(dataset.copy(),sr_tor_all*3,'orglc')\n",
    "orglc_mk_hr_ckm=combine(dataset.copy(),mk_hr_ckm_all*3,'orglc')\n",
    "\n",
    "orglc_omix_sr_tor=combine(dataset.copy(),sr_tor_all*3,'orglc_omix')\n",
    "orglc_omix_mk_hr_ckm=combine(dataset.copy(),mk_hr_ckm_all*3,'orglc_omix')\n",
    "\n",
    "\n",
    "\n",
    "orgl_sr_tor=combine(dataset.copy(),sr_tor_latn*6,'orgl')\n",
    "orgl_mk_hr_ckm=combine(dataset.copy(),mk_hr_ckm_latn*4,'orgl')\n",
    "orgl_sl_cer=combine(dataset.copy(),sl_cer_latn*6,'orgl')\n",
    "orgl_hr_ckm=combine(dataset.copy(),hr_ckm_latn*6,'orgl')\n",
    "\n",
    "orgl_omix_sr_tor=combine(dataset.copy(),sr_tor_latn*6,'orglc_omix')\n",
    "orgl_omix_mk_hr_ckm=combine(dataset.copy(),mk_hr_ckm_latn*4,'orglc_omix')\n",
    "orgl_omix_sl_cer=combine(dataset.copy(),sl_cer_latn*6,'orglc_omix')\n",
    "orgl_omix_hr_ckm=combine(dataset.copy(),hr_ckm_latn*6,'orglc_omix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98b4e94-fd75-42f5-bc79-fe0c0bdf3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dict={\n",
    "    'orglc':orglc,\n",
    "    'orgl':orgl,\n",
    "    'orglc_omix':orglc_omix,\n",
    "    'orgl_omix':orgl_omix,\n",
    "    'orglc_sr_tor':orglc_sr_tor,\n",
    "    'orglc_mk_hr_ckm':orglc_mk_hr_ckm,\n",
    "    'orglc_omix_sr_tor':orglc_omix_sr_tor,\n",
    "    'orglc_omix_mk_hr_ckm':orglc_omix_mk_hr_ckm,\n",
    "    'orgl_sr_tor':orgl_sr_tor,\n",
    "    'orgl_mk_hr_ckm':orgl_mk_hr_ckm,\n",
    "    'orgl_sl_cer':orgl_sl_cer,\n",
    "    'orgl_hr_ckm':orgl_hr_ckm,\n",
    "    'orgl_omix_sr_tor':orgl_omix_sr_tor,\n",
    "    'orgl_omix_mk_hr_ckm':orgl_omix_mk_hr_ckm,\n",
    "    'orgl_omix_sl_cer':orgl_omix_sl_cer,\n",
    "    'orgl_omix_hr_ckm':orgl_omix_hr_ckm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce98ae18-3254-4f6f-ad70-906e4acda307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "hf_dataset={'train':[]}\n",
    "for dname,mdata in all_data_dict.items():\n",
    "    hf_dataset['train'] = Dataset.from_pandas(pd.DataFrame(data=mdata))\n",
    "    hf_dataset = DatasetDict(hf_dataset)\n",
    "    hf_dataset=hf_dataset.shuffle(seed=41)\n",
    "    hf_dataset.save_to_disk(os.path.join('../data',dname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cafc36-609f-4e18-8750-9e0ece69da70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27455d74-c2a0-4e90-93c0-fb8f254db828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orglc', 'orgl', 'orglc_omix', 'orgl_omix', 'orglc_sr_tor', 'orglc_mk_hr_ckm', 'orglc_omix_sr_tor', 'orglc_omix_mk_hr_ckm', 'orgl_sr_tor', 'orgl_mk_hr_ckm', 'orgl_sl_cer', 'orgl_hr_ckm', 'orgl_omix_sr_tor', 'orgl_omix_mk_hr_ckm', 'orgl_omix_sl_cer', 'orgl_omix_hr_ckm'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c8ec6409-dc82-4f31-8d9d-e53ab176ecb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2963fda4-5c28-4ed9-a469-cc74752adae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.trans',\n",
       " 'train',\n",
       " 'train-genx.trans',\n",
       " 'train-reverse.trans',\n",
       " 'train.cyrl',\n",
       " 'train-reverse',\n",
       " 'train-reverse.cyrl',\n",
       " 'train-genx',\n",
       " 'train-genx.cyrl']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d77153-522a-4e97-ac9b-542a4637d293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnv_copa",
   "language": "python",
   "name": "vnv_copa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
